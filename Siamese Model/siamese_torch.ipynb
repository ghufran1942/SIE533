{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "2Tr6AjrnKsbB"
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'tensorflow'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 5\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodel_selection\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m train_test_split\n\u001b[1;32m----> 5\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mtf\u001b[39;00m\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mlayers\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;241m*\u001b[39m\n\u001b[0;32m      7\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodels\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Sequential, Model\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'tensorflow'"
     ]
    }
   ],
   "source": [
    "#### Import necessary packages/functions\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import *\n",
    "from tensorflow.keras.models import Sequential, Model\n",
    "from nltk.corpus import stopwords\n",
    "import string\n",
    "from unidecode import unidecode\n",
    "import random\n",
    "import itertools\n",
    "import csv\n",
    "from tqdm import tqdm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "qQHJE9USKuxR"
   },
   "outputs": [],
   "source": [
    "PATH = \"../data/\"\n",
    "\n",
    "#### Reading in the data.\n",
    "content = pd.read_csv(f\"{PATH}content_filtered.csv\")\n",
    "correlations = pd.read_csv(f\"{PATH}correlations.csv\")\n",
    "#sample_submission = pd.read_csv(\"/kaggle/input/learning-equality-curriculum-recommendations/sample_submission.csv\")\n",
    "topics = pd.read_csv(f\"{PATH}topics_filtered.csv\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "p0zJcO-_LU1a"
   },
   "outputs": [],
   "source": [
    "#### Create combine function\n",
    "def combine(correlations, topics, content):\n",
    "    '''\n",
    "    - Inputs our three datasets and combines the topic/content information with the topic/content correlations data. \n",
    "    - All topic/content information is concatenated to one \"features\" column, which includes the language, title, description, etc.\n",
    "    - Output includes the correlations topics information, correlations content information, and a dictionary to convert indices to their\n",
    "      corresponding topic/content id. \n",
    "    '''\n",
    "    #Drop/combine columns\n",
    "    content[\"text\"] = content[\"text\"].fillna('')\n",
    "    content = content.dropna()\n",
    "    content_combined = content[\"language\"] + \" \" + content[\"title\"] + \" \" + content[\"description\"] + \" \" + content[\"text\"]\n",
    "    content_combined = pd.DataFrame({\"id\":content[\"id\"], \"features\":content_combined})\n",
    "    print(\"content_combined\", content_combined.shape)\n",
    "\n",
    "    topics[\"description\"] = topics[\"description\"].fillna('')\n",
    "    topics = topics.dropna()\n",
    "    topics_combined = topics[\"language\"] + \" \" + topics[\"channel\"] + ' ' + topics[\"title\"] + \" \" + topics[\"description\"]\n",
    "    topics_combined = pd.DataFrame({\"id\":topics[\"id\"], \"features\":topics_combined})\n",
    "    print(\"topics_combined\", topics_combined.shape)\n",
    "    \n",
    "    #Explode correlations rows\n",
    "    correlations[\"content_ids\"] = correlations[\"content_ids\"].str.split()\n",
    "    correlations = correlations.explode(\"content_ids\")\n",
    "\n",
    "    #Merge\n",
    "    merged = correlations.merge(topics_combined, how=\"inner\", left_on=\"topic_id\", right_on=\"id\")\n",
    "    print(\"merged\", merged.shape)\n",
    "    merged = merged.reset_index().merge(content_combined, how=\"inner\", left_on=\"content_ids\", right_on=\"id\", sort=False, suffixes=(\"_topics\", \"_content\")).sort_values(axis=0, by=\"index\")\n",
    "    merged = merged.drop([\"content_ids\", \"topic_id\"], axis=1)\n",
    "    print(\"merged\", merged.shape)\n",
    "\n",
    "    #Split\n",
    "    corr_topics = merged[['index', 'features_topics']]\n",
    "    corr_topics.columns = ['id', 'features']\n",
    "    corr_content = merged[['index', 'features_content']]\n",
    "    corr_content.columns = ['id', 'features']\n",
    "\n",
    "    index_to_topic = pd.Series(merged.id_topics.values, index=merged.index).to_dict()\n",
    "    index_to_content = pd.Series(merged.id_content.values, index=merged.index).to_dict()\n",
    "\n",
    "    return corr_topics, corr_content, index_to_topic, index_to_content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Atm-OtRKLYQB",
    "outputId": "f8c0185a-c3d0-4099-9a0a-a42c62ba317d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "content_combined (16906, 2)\n",
      "topics_combined (36078, 2)\n",
      "merged (127725, 4)\n",
      "merged (47435, 5)\n"
     ]
    }
   ],
   "source": [
    "#### Apply combine() to our data\n",
    "corr_topics, corr_content, index_to_topic, index_to_content = combine(correlations, topics, content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "4KghXEypLaqK",
    "outputId": "9ff863bc-7065-4c6d-8715-4fefa90cb10e"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\ghufr\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "#### Create a stopword removal function to remove stopwords for each language\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "# Dictionary of languages found in our data\n",
    "lang_dict = {\n",
    "    \"en\":\"english\",\n",
    "    \"es\":\"spanish\",\n",
    "    \"it\":\"italian\",\n",
    "    'pt':\"portuguese\",\n",
    "    'mr':'marathi',\n",
    "    'bg':'bulgarian',\n",
    "    'gu':'gujarati',\n",
    "    'sw':'swahili',\n",
    "    'hi':'hindi',\n",
    "    'ar':'arabic',\n",
    "    'bn':'bengali',\n",
    "    'as':'assamese',\n",
    "    'zh':'chinese',\n",
    "    'fr':'french',\n",
    "    'km':'khmer',\n",
    "    'pl':'polish',\n",
    "    'ta':'tamil',\n",
    "    'or':'oriya',\n",
    "    'ru':'russian',\n",
    "    'kn':'kannada',\n",
    "    'swa':'swahili',\n",
    "    'my':'burmese',\n",
    "    'pnb':'punjabi',\n",
    "    'fil':'filipino',\n",
    "    'tr':'turkish',\n",
    "    'te':'telugu',\n",
    "    'ur':'urdu',\n",
    "    'fi':'finnish',\n",
    "    'pn':'unknown',\n",
    "    'mu':'unknown'}\n",
    "\n",
    "# List of languages supported by the natural language tool kit (NLTK) module.\n",
    "supported_languages = stopwords.fileids()\n",
    "\n",
    "def remove_stopwords(text):\n",
    "    '''\n",
    "    Checks language of text then removes stopwords from that language if supported.\n",
    "    '''\n",
    "    lang_code = text[0:2]\n",
    "    if lang_dict[lang_code] in supported_languages:\n",
    "        for word in stopwords.words(lang_dict[lang_code]):\n",
    "            text = text.replace(' ' + word + ' ', ' ')\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "H55Z_xzzLdK_"
   },
   "outputs": [],
   "source": [
    "#### Apply remove_stopwords() to our data\n",
    "corr_topics[\"features\"] = corr_topics.features.apply(remove_stopwords)\n",
    "corr_content[\"features\"] = corr_content.features.apply(remove_stopwords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "GEVFT2uALvu1"
   },
   "outputs": [],
   "source": [
    "#### Create train/test indices for our data \n",
    "random.seed(10)\n",
    "train_indices = random.sample(range(len(corr_content)), round(0.8*len(corr_content))) #80/20 train/test split\n",
    "\n",
    "#### Split training data so 50% is matching and 50% is not matching\n",
    "half = round(len(train_indices) / 2)\n",
    "full = len(train_indices)\n",
    "\n",
    "train_topics_half = corr_topics.iloc[train_indices[:half], :]\n",
    "train_content_half = corr_content.iloc[train_indices[:half], :]\n",
    "\n",
    "#Shift second half so that topics/content are not matching\n",
    "train_topics_full = corr_topics.iloc[train_indices[half:(full-20)], :] \n",
    "train_content_full = corr_content.iloc[train_indices[(half+20):(full)], :] \n",
    "\n",
    "train_topics = pd.concat([train_topics_half, train_topics_full]).reset_index().drop(\"index\", axis=1)\n",
    "train_content = pd.concat([train_content_half, train_content_full]).reset_index().drop(\"index\", axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "Rb1I1pTNLyV-"
   },
   "outputs": [],
   "source": [
    "#### Repeat for test data\n",
    "test_topics = corr_topics.drop(train_indices, axis=0)\n",
    "test_content = corr_content.drop(train_indices, axis=0)\n",
    "\n",
    "half = round(len(test_topics.features) / 2)\n",
    "full = len(test_topics.features)\n",
    "\n",
    "test_topics_half = test_topics.iloc[:half, :]\n",
    "test_content_half = test_content.iloc[:half, :]\n",
    "\n",
    "test_topics_full = test_topics.iloc[half:(full - 5), :]\n",
    "test_content_full = test_content.iloc[(half+5):full, :]\n",
    "\n",
    "test_topics = pd.concat([test_topics_half, test_topics_full]).reset_index().drop(\"index\", axis=1)\n",
    "test_content = pd.concat([test_content_half, test_content_full]).reset_index().drop(\"index\", axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "breawo_9Lz6x"
   },
   "outputs": [],
   "source": [
    "#### Create labels\n",
    "train_labels = np.array((train_topics.id == train_content.id).astype(int))\n",
    "test_labels = np.array((test_topics.id == test_content.id).astype(int))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "GsMMgzQ3L2XV"
   },
   "outputs": [],
   "source": [
    "#### Convert data to tensors\n",
    "train_topics = tf.data.Dataset.from_tensor_slices(tf.cast(train_topics.features, tf.string))\n",
    "train_content = tf.data.Dataset.from_tensor_slices(tf.cast(train_content.features, tf.string))\n",
    "train_labels = tf.data.Dataset.from_tensor_slices(tf.cast(train_labels, tf.int32))\n",
    "\n",
    "test_topics = tf.data.Dataset.from_tensor_slices(tf.cast(test_topics.features, tf.string))\n",
    "test_content = tf.data.Dataset.from_tensor_slices(tf.cast(test_content.features, tf.string))\n",
    "test_labels = tf.data.Dataset.from_tensor_slices(tf.cast(test_labels, tf.int32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "JPLN-m0oL3rH"
   },
   "outputs": [],
   "source": [
    "#### Combine data into TensorFlow Datasets\n",
    "#### Perfectly shuffle, batch, cache, and prefetch our new datasets\n",
    "train_ds = tf.data.Dataset.zip(\n",
    "    ((train_topics, train_content), train_labels)\n",
    ")\n",
    "\n",
    "train_ds = train_ds.shuffle(buffer_size = train_ds.cardinality().numpy()).batch(batch_size = 64).cache().prefetch(tf.data.experimental.AUTOTUNE)\n",
    "\n",
    "test_ds = tf.data.Dataset.zip(\n",
    "    ((test_topics, test_content), test_labels)\n",
    ")\n",
    "\n",
    "test_ds = test_ds.shuffle(buffer_size = test_ds.cardinality().numpy()).batch(batch_size = 64).cache().prefetch(tf.data.experimental.AUTOTUNE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "nZmh8LUxL6Ci"
   },
   "outputs": [],
   "source": [
    "#### Create Text Vectorization Layer\n",
    "# Hyperparameters\n",
    "VOCAB_SIZE = 1000000\n",
    "MAX_LEN = 50\n",
    "\n",
    "def my_standardize(text): \n",
    "    '''\n",
    "    A text standardization function that is applied for every element in the vectorize layer. \n",
    "    '''\n",
    "    text = tf.strings.lower(text, encoding='utf-8') #lowercase\n",
    "    text = tf.strings.regex_replace(text, f\"([{string.punctuation}])\", r\" \") #remove punctuation\n",
    "    text = tf.strings.regex_replace(text, '\\n', \"\") #remove newlines\n",
    "    text = tf.strings.regex_replace(text, ' +', \" \") #remove 2+ whitespaces\n",
    "    text = tf.strings.strip(text) #remove leading and tailing whitespaces\n",
    "    return text\n",
    "\n",
    "vectorize_layer = TextVectorization(\n",
    "    standardize = my_standardize,\n",
    "    split = \"whitespace\",\n",
    "    max_tokens = VOCAB_SIZE + 2,\n",
    "    output_mode = 'int',\n",
    "    output_sequence_length = MAX_LEN\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "egV0jjhJMCuc"
   },
   "outputs": [],
   "source": [
    "#### Adapt text vectorization layer to our data\n",
    "vectorize_layer.adapt(pd.concat([corr_topics[\"features\"], corr_content[\"features\"]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "4KWrcJdAMEj4",
    "outputId": "11de4bc7-2493-4a04-b0af-f9d3f71f8e8a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_1 (InputLayer)           [(None, 1)]          0           []                               \n",
      "                                                                                                  \n",
      " input_2 (InputLayer)           [(None, 1)]          0           []                               \n",
      "                                                                                                  \n",
      " text_vectorization (TextVector  (None, 50)          0           ['input_1[0][0]',                \n",
      " ization)                                                         'input_2[0][0]']                \n",
      "                                                                                                  \n",
      " sequential (Sequential)        (None, 128)          256032896   ['text_vectorization[1][0]',     \n",
      "                                                                  'text_vectorization[0][0]']     \n",
      "                                                                                                  \n",
      " concatenate (Concatenate)      (None, 256)          0           ['sequential[1][0]',             \n",
      "                                                                  'sequential[0][0]']             \n",
      "                                                                                                  \n",
      " dense_1 (Dense)                (None, 64)           16448       ['concatenate[0][0]']            \n",
      "                                                                                                  \n",
      " dense_2 (Dense)                (None, 1)            65          ['dense_1[0][0]']                \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 256,049,409\n",
      "Trainable params: 256,049,409\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "inp_topics = Input((1, ), dtype=tf.string)\n",
    "inp_content = Input((1, ), dtype=tf.string)\n",
    "\n",
    "vectorized_topics = vectorize_layer(inp_topics)\n",
    "vectorized_content = vectorize_layer(inp_content)\n",
    "\n",
    "snn = Sequential([ \n",
    "  Embedding(VOCAB_SIZE, 256),\n",
    "  GlobalAveragePooling1D(),\n",
    "  Flatten(),\n",
    "  Dense(128, activation='relu'),\n",
    "])\n",
    "\n",
    "snn_content = snn(vectorized_content)\n",
    "snn_topics = snn(vectorized_topics)\n",
    "\n",
    "concat = Concatenate()([snn_topics, snn_content])\n",
    "\n",
    "dense = Dense(64, activation='relu')(concat)\n",
    "\n",
    "output = Dense(1, activation='sigmoid')(dense)\n",
    "\n",
    "model = Model(inputs=[inp_topics, inp_content], outputs=output)\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "xBoGYP_RMHFz"
   },
   "outputs": [],
   "source": [
    "model.compile(loss='binary_crossentropy',\n",
    "              optimizer='adam',\n",
    "              metrics=tf.keras.metrics.AUC())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "XgIbcNk8MKwf",
    "outputId": "43500c5d-0904-4f34-f8f0-9869030d4561"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "593/593 [==============================] - 441s 742ms/step - loss: 0.5983 - auc: 0.7112\n",
      "Epoch 2/5\n",
      "593/593 [==============================] - 433s 730ms/step - loss: 0.4182 - auc: 0.8748\n",
      "Epoch 3/5\n",
      "593/593 [==============================] - 430s 725ms/step - loss: 0.3179 - auc: 0.9316\n",
      "Epoch 4/5\n",
      "593/593 [==============================] - 442s 745ms/step - loss: 0.2599 - auc: 0.9559\n",
      "Epoch 5/5\n",
      " 79/593 [==>...........................] - ETA: 6:18 - loss: 0.2149 - auc: 0.9695"
     ]
    }
   ],
   "source": [
    "model.fit(train_ds, epochs=5, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bFh3K_bqMOIl"
   },
   "outputs": [],
   "source": [
    "model.evaluate(test_ds, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JEvE7OAaMOwP"
   },
   "outputs": [],
   "source": [
    "#### Antijoin topics with correlations data, since we don't have to predict those topics\n",
    "outer_joined = topics.merge(correlations, how='outer', left_on='id', right_on='topic_id', indicator=True)\n",
    "topics = outer_joined[(outer_joined._merge == 'left_only')].drop('_merge', axis=1)\n",
    "\n",
    "#### Fill missing values and concatenate text to features column \n",
    "topics = topics.fillna(\"\")\n",
    "topics_ids = topics.id.values\n",
    "topics_lang = topics.language\n",
    "topics_index = topics.index\n",
    "topics_features = topics[\"language\"] + ' ' + topics[\"channel\"] + ' ' + topics[\"title\"] + ' ' + topics[\"description\"]\n",
    "del topics\n",
    "\n",
    "#### Repeat for content, except we keep all content data\n",
    "content = content.fillna(\"\")\n",
    "content_ids = content.id.values\n",
    "content_index = content.index\n",
    "content_lang = content.language\n",
    "content_features = content[\"language\"] + ' ' + content[\"title\"] + ' ' + content[\"description\"] + ' ' + content[\"text\"]\n",
    "del content\n",
    "\n",
    "index_to_content = pd.Series(content_ids, index=content_index).to_dict()\n",
    "index_to_topic = pd.Series(topics_ids, index=topics_index).to_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6R6ri9SvMRXR"
   },
   "outputs": [],
   "source": [
    "#### Remove stopwords\n",
    "topics_features = topics_features.apply(remove_stopwords)\n",
    "content_features = content_features.apply(remove_stopwords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7NuFiXvFMVwG"
   },
   "outputs": [],
   "source": [
    "#### Write predictions to output_file\n",
    "THRESHOLD = 0.99994\n",
    "\n",
    "output_file = \"submission.csv\"\n",
    "f = open(output_file, 'w')\n",
    "\n",
    "writer = csv.writer(f)\n",
    "writer.writerow([\"topic_id\", \"content_ids\"])\n",
    "\n",
    "for i in tqdm(topics_features.index):\n",
    "    temp_content = tf.data.Dataset.from_tensor_slices(\n",
    "        tf.cast(content_features[content_lang == topics_lang[i]], tf.string)\n",
    "    )\n",
    "    temp_topic = tf.data.Dataset.from_tensor_slices(\n",
    "        tf.cast(np.repeat(topics_features[i], len(temp_content)), tf.string)\n",
    "    )\n",
    "    temp_ds = tf.data.Dataset.zip(((temp_topic, temp_content), ))\\\n",
    "        .batch(batch_size=64)\\\n",
    "            .cache()\\\n",
    "                .prefetch(tf.data.experimental.AUTOTUNE)\n",
    "    matches = model.predict(temp_ds, verbose=0)\n",
    "    matches = [i for i in range(len(matches)) if matches[i] > THRESHOLD]\n",
    "    matches = \" \".join([index_to_content[x] for x in matches])\n",
    "    writer.writerow([index_to_topic[i], matches])\n",
    "\n",
    "#### Add given correlations data\n",
    "writer.writerows([correlations.topic_id, correlations.content_ids])    \n",
    "\n",
    "f.close()"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
