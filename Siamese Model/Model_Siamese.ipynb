{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d5fbbf6e-613a-43da-be8c-26f32f4b999c",
   "metadata": {},
   "source": [
    "### Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "87523a74-a55d-412a-9e37-1d0d696fd56b",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'tensorflow'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[12], line 4\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodel_selection\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m train_test_split\n\u001b[1;32m----> 4\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mtf\u001b[39;00m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mlayers\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;241m*\u001b[39m\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodels\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Sequential, Model\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'tensorflow'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearnx.model_selection import train_test_split\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import *\n",
    "from tensorflow.keras.models import Sequential, Model\n",
    "from nltk.corpus import stopwords\n",
    "import string\n",
    "from unidecode import unidecode\n",
    "import random\n",
    "import itertools\n",
    "import csv"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "beefedd9-b3b4-4763-835e-255209f1f5a7",
   "metadata": {},
   "source": [
    "### Import Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "164f7014-1215-44e9-8266-2919fb12cccd",
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH = \"../data/\"\n",
    "content = pd.read_csv(f\"{PATH}content_filtered.csv\")\n",
    "topics = pd.read_csv(f\"{PATH}topics_filtered.csv\")\n",
    "correlations = pd.read_csv(f\"{PATH}correlations.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0218909b-5a42-4625-be9e-1807bebfaec9",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Create combine function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4f46e642-280a-46be-8748-30eeebbbd4ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "def combine(correlations, topics, content):\n",
    "    '''\n",
    "    - Inputs our three datasets and combines the topic/content information with the topic/content correlations data.\n",
    "    - All topic/content information is concatenated to one \"features\" column, which includes the language, title, description, etc.\n",
    "    - Output includes the correlations topics information, correlations content information, and a dictionary to convert indices to their\n",
    "      corresponding topic/content id.\n",
    "    '''\n",
    "    #Drop/combine columns\n",
    "    content[\"text\"] = content[\"text\"].fillna('')\n",
    "    content = content.dropna()\n",
    "    content_combined = content[\"language\"] + \" \" + content[\"title\"] + \" \" + content[\"description\"] + \" \" + content[\"text\"]\n",
    "    content_combined = pd.DataFrame({\"id\":content[\"id\"], \"features\":content_combined})\n",
    "\n",
    "    topics[\"description\"] = topics[\"description\"].fillna('')\n",
    "    topics = topics.dropna()\n",
    "    topics_combined = topics[\"language\"] + \" \" + topics[\"channel\"] + ' ' + topics[\"title\"] + \" \" + topics[\"description\"]\n",
    "    topics_combined = pd.DataFrame({\"id\":topics[\"id\"], \"features\":topics_combined})\n",
    "\n",
    "    #Explode correlations rows\n",
    "    correlations[\"content_ids\"] = correlations[\"content_ids\"].str.split()\n",
    "    correlations = correlations.explode(\"content_ids\")\n",
    "\n",
    "    #Merge\n",
    "    merged = correlations.merge(topics_combined, how=\"inner\", left_on=\"topic_id\", right_on=\"id\")\n",
    "    merged = merged.reset_index().merge(content_combined, how=\"inner\", left_on=\"content_ids\", right_on=\"id\", sort=False, suffixes=(\"_topics\", \"_content\")).sort_values(axis=0, by=\"index\")\n",
    "    merged = merged.drop([\"content_ids\", \"topic_id\"], axis=1)\n",
    "\n",
    "    #Split\n",
    "    corr_topics = merged[['index', 'features_topics']]\n",
    "    corr_topics.columns = ['id', 'features']\n",
    "    corr_content = merged[['index', 'features_content']]\n",
    "    corr_content.columns = ['id', 'features']\n",
    "\n",
    "    index_to_topic = pd.Series(merged.id_topics.values, index=merged.index).to_dict()\n",
    "    index_to_content = pd.Series(merged.id_content.values, index=merged.index).to_dict()\n",
    "\n",
    "    return corr_topics, corr_content, index_to_topic, index_to_content"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "810407df-1797-41dd-81a3-e8e7dc93ef4f",
   "metadata": {},
   "source": [
    "#### Remove Stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3c5dcba7-d6a4-4f64-aa8a-f268193f7fac",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_stopwords(text):\n",
    "    '''\n",
    "    Checks language of text then removes stopwords from that language if supported.\n",
    "    '''\n",
    "    supported_languages = stopwords.fileids()\n",
    "    lang_code = text[0:2]\n",
    "    for word in stopwords.words(lang_dict[lang_code]):\n",
    "        text = text.replace(' ' + word + ' ', ' ')\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7f603aa-be6f-4d39-8ee3-e34fb834e986",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
