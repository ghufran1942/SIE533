{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a4acf1ca-766d-4853-95e4-53fec5ed64bd",
   "metadata": {},
   "source": [
    "## Required Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "bfa28352-a7de-45e5-81c4-81481d6fc51c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "import torch\n",
    "from nltk.corpus import stopwords\n",
    "import string\n",
    "from unidecode import unidecode\n",
    "import random\n",
    "import itertools\n",
    "import csv"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e05e8f54-8476-4668-9d5b-d27c2c883101",
   "metadata": {},
   "source": [
    "## Import Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c581e8bc-e50d-4feb-9bd8-f3ba1f38a8d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH = '../data/'\n",
    "\n",
    "content = pd.read_csv(f\"{PATH}content_filtered.csv\")\n",
    "correlations = pd.read_csv(f\"{PATH}correlations.csv\")\n",
    "#sample_submission = pd.read_csv(\"/kaggle/input/learning-equality-curriculum-recommendations/sample_submission.csv\")\n",
    "topics = pd.read_csv(f\"{PATH}topics_filtered.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0034ec4-9115-46ea-8cd4-8854d47807f8",
   "metadata": {},
   "source": [
    "## Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "75db3c32-70a8-454f-8003-3d6c4072589e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "content_combined (16906, 2)\n",
      "topics_combined (36078, 2)\n",
      "merged (127725, 4)\n",
      "merged (47435, 5)\n"
     ]
    }
   ],
   "source": [
    "def combine(correlations, topics, content):\n",
    "    '''\n",
    "    - Inputs our three datasets and combines the topic/content information with the topic/content correlations data. \n",
    "    - All topic/content information is concatenated to one \"features\" column, which includes the language, title, description, etc.\n",
    "    - Output includes the correlations topics information, correlations content information, and a dictionary to convert indices to their\n",
    "      corresponding topic/content id. \n",
    "    '''\n",
    "    #Drop/combine columns\n",
    "    content[\"text\"] = content[\"text\"].fillna('')\n",
    "    content = content.dropna()\n",
    "    content_combined = content[\"language\"] + \" \" + content[\"title\"] + \" \" + content[\"description\"] + \" \" + content[\"text\"]\n",
    "    content_combined = pd.DataFrame({\"id\":content[\"id\"], \"features\":content_combined})\n",
    "    print(\"content_combined\", content_combined.shape)\n",
    "\n",
    "    topics[\"description\"] = topics[\"description\"].fillna('')\n",
    "    topics = topics.dropna()\n",
    "    topics_combined = topics[\"language\"] + \" \" + topics[\"channel\"] + ' ' + topics[\"title\"] + \" \" + topics[\"description\"]\n",
    "    topics_combined = pd.DataFrame({\"id\":topics[\"id\"], \"features\":topics_combined})\n",
    "    print(\"topics_combined\", topics_combined.shape)\n",
    "    \n",
    "    #Explode correlations rows\n",
    "    correlations[\"content_ids\"] = correlations[\"content_ids\"].str.split()\n",
    "    correlations = correlations.explode(\"content_ids\")\n",
    "\n",
    "    #Merge\n",
    "    merged = correlations.merge(topics_combined, how=\"inner\", left_on=\"topic_id\", right_on=\"id\")\n",
    "    print(\"merged\", merged.shape)\n",
    "    merged = merged.reset_index().merge(content_combined, how=\"inner\", left_on=\"content_ids\", right_on=\"id\", sort=False, suffixes=(\"_topics\", \"_content\")).sort_values(axis=0, by=\"index\")\n",
    "    merged = merged.drop([\"content_ids\", \"topic_id\"], axis=1)\n",
    "    print(\"merged\", merged.shape)\n",
    "\n",
    "    #Split\n",
    "    corr_topics = merged[['index', 'features_topics']]\n",
    "    corr_topics.columns = ['id', 'features']\n",
    "    corr_content = merged[['index', 'features_content']]\n",
    "    corr_content.columns = ['id', 'features']\n",
    "\n",
    "    index_to_topic = pd.Series(merged.id_topics.values, index=merged.index).to_dict()\n",
    "    index_to_content = pd.Series(merged.id_content.values, index=merged.index).to_dict()\n",
    "\n",
    "    return corr_topics, corr_content, index_to_topic, index_to_content\n",
    "\n",
    "#### Apply combine() to our data\n",
    "corr_topics, corr_content, index_to_topic, index_to_content = combine(correlations, topics, content)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d4aecd56-6e56-4abc-aa03-027b702924fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "lang_dict = {\"en\":\"english\"}\n",
    "\n",
    "# List of languages supported by the natural language tool kit (NLTK) module.\n",
    "supported_languages = stopwords.fileids()\n",
    "\n",
    "def remove_stopwords(text):\n",
    "    '''\n",
    "    Checks language of text then removes stopwords from that language if supported.\n",
    "    '''\n",
    "    lang_code = text[0:2]\n",
    "    if lang_dict[lang_code] in supported_languages:\n",
    "        for word in stopwords.words(lang_dict[lang_code]):\n",
    "            text = text.replace(' ' + word + ' ', ' ')\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "951fb269-5766-49a1-9875-72a06f2e99d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "corr_topics[\"features\"] = corr_topics.features.apply(remove_stopwords)\n",
    "corr_content[\"features\"] = corr_content.features.apply(remove_stopwords)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce01f967-ce97-4694-8026-dcf0707fcd36",
   "metadata": {},
   "source": [
    "## Create Training and Testing sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9ecb6b87-a651-4b40-b207-cae0303d453a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training\n",
    "random.seed(10)\n",
    "train_indices = random.sample(range(len(corr_content)), round(0.8*len(corr_content))) #80/20 train/test split\n",
    "\n",
    "half = round(len(train_indices) / 2)\n",
    "full = len(train_indices)\n",
    "\n",
    "train_topics_half = corr_topics.iloc[train_indices[:half], :]\n",
    "train_content_half = corr_content.iloc[train_indices[:half], :]\n",
    "\n",
    "train_topics_full = corr_topics.iloc[train_indices[half:(full-20)], :] \n",
    "train_content_full = corr_content.iloc[train_indices[(half+20):(full)], :] \n",
    "\n",
    "train_topics = pd.concat([train_topics_half, train_topics_full]).reset_index().drop(\"index\", axis=1)\n",
    "train_content = pd.concat([train_content_half, train_content_full]).reset_index().drop(\"index\", axis=1)\n",
    "\n",
    "# Testing\n",
    "test_topics = corr_topics.drop(train_indices, axis=0)\n",
    "test_content = corr_content.drop(train_indices, axis=0)\n",
    "\n",
    "half = round(len(test_topics.features) / 2)\n",
    "full = len(test_topics.features)\n",
    "\n",
    "test_topics_half = test_topics.iloc[:half, :]\n",
    "test_content_half = test_content.iloc[:half, :]\n",
    "\n",
    "test_topics_full = test_topics.iloc[half:(full - 5), :]\n",
    "test_content_full = test_content.iloc[(half+5):full, :]\n",
    "\n",
    "test_topics = pd.concat([test_topics_half, test_topics_full]).reset_index().drop(\"index\", axis=1)\n",
    "test_content = pd.concat([test_content_half, test_content_full]).reset_index().drop(\"index\", axis=1)\n",
    "\n",
    "# Create Train Labels\n",
    "train_labels = np.array((train_topics.id == train_content.id).astype(int))\n",
    "test_labels = np.array((test_topics.id == test_content.id).astype(int))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d40f2c3b-dee6-4f4e-a392-cde0bc09bdb5",
   "metadata": {},
   "source": [
    "## Conversion to Tensors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "817c052f-5454-4563-a704-5c602f1bcf93",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "type object 'Dataset' has no attribute 'from_tensor_slices'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[14], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m train_topics \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mutils\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdata\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mDataset\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_tensor_slices\u001b[49m(torch\u001b[38;5;241m.\u001b[39mas_tensor(train_topics\u001b[38;5;241m.\u001b[39mfeatures, dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mstring))\n\u001b[1;32m      2\u001b[0m train_topics \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mutils\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39mDataset\u001b[38;5;241m.\u001b[39mfrom_tensor_slices(torch\u001b[38;5;241m.\u001b[39mas_tensor(train_content\u001b[38;5;241m.\u001b[39mfeatures, dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mstring))\n\u001b[1;32m      3\u001b[0m train_topics \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mutils\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39mDataset\u001b[38;5;241m.\u001b[39mfrom_tensor_slices(torch\u001b[38;5;241m.\u001b[39mas_tensor(train_labels\u001b[38;5;241m.\u001b[39mfeatures, dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mstring))\n",
      "\u001b[0;31mAttributeError\u001b[0m: type object 'Dataset' has no attribute 'from_tensor_slices'"
     ]
    }
   ],
   "source": [
    "train_topics = torch.utils.data.TensorDataset(torch.as_tensor(train_topics.features, dtype=torch.string))\n",
    "train_topics = torch.utils.data.Dataset.from_tensor_slices(torch.as_tensor(train_content.features, dtype=torch.string))\n",
    "train_topics = torch.utils.data.Dataset.from_tensor_slices(torch.as_tensor(train_labels.features, dtype=torch.string))\n",
    "# train_topics = tf.data.Dataset.from_tensor_slices(tf.cast(train_topics.features, tf.string))\n",
    "# train_content = tf.data.Dataset.from_tensor_slices(tf.cast(train_content.features, tf.string))\n",
    "# train_labels = tf.data.Dataset.from_tensor_slices(tf.cast(train_labels, tf.int32))\n",
    "\n",
    "test_topics = torch.utils.data.Dataset.from_tensor_slices(torch.as_tensor(test_topics.features, dtype=torch.string))\n",
    "test_topics = torch.utils.data.Dataset.from_tensor_slices(torch.as_tensor(test_content.features, dtype=torch.string))\n",
    "test_topics = torch.utils.data.Dataset.from_tensor_slices(torch.as_tensor(test_labels.features, dtype=torch.string))\n",
    "# test_topics = tf.data.Dataset.from_tensor_slices(tf.cast(test_topics.features, tf.string))\n",
    "# test_content = tf.data.Dataset.from_tensor_slices(tf.cast(test_content.features, tf.string))\n",
    "# test_labels = tf.data.Dataset.from_tensor_slices(tf.cast(test_labels, tf.int32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73a90bcb-9865-478c-879a-aa2c2cd1b717",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60b503af-bc2e-41ed-b1a7-77080d99d851",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
