{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bFjK2uZYKm0N",
        "outputId": "dab213f7-903c-4545-cb57-2d2666f0f595"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting unidecode\n",
            "  Downloading Unidecode-1.3.6-py3-none-any.whl (235 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m235.9/235.9 kB\u001b[0m \u001b[31m3.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: unidecode\n",
            "Successfully installed unidecode-1.3.6\n"
          ]
        }
      ],
      "source": [
        "!pip install unidecode"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#### Import necessary packages/functions\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.layers import *\n",
        "from tensorflow.keras.models import Sequential, Model\n",
        "from nltk.corpus import stopwords\n",
        "import string\n",
        "from unidecode import unidecode\n",
        "import random\n",
        "import itertools\n",
        "import csv\n",
        "\n"
      ],
      "metadata": {
        "id": "2Tr6AjrnKsbB"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "PATH = \"/content/drive/MyDrive/School/Masters/Study/Spring 2023/SIE533/Project/SIE533/data/\"\n",
        "\n",
        "#### Reading in the data.\n",
        "content = pd.read_csv(f\"{PATH}content_filtered.csv\")\n",
        "correlations = pd.read_csv(f\"{PATH}correlations.csv\")\n",
        "#sample_submission = pd.read_csv(\"/kaggle/input/learning-equality-curriculum-recommendations/sample_submission.csv\")\n",
        "topics = pd.read_csv(f\"{PATH}topics.csv\")\n",
        "\n"
      ],
      "metadata": {
        "id": "qQHJE9USKuxR"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#### Create combine function\n",
        "def combine(correlations, topics, content):\n",
        "    '''\n",
        "    - Inputs our three datasets and combines the topic/content information with the topic/content correlations data. \n",
        "    - All topic/content information is concatenated to one \"features\" column, which includes the language, title, description, etc.\n",
        "    - Output includes the correlations topics information, correlations content information, and a dictionary to convert indices to their\n",
        "      corresponding topic/content id. \n",
        "    '''\n",
        "    #Drop/combine columns\n",
        "    content[\"text\"] = content[\"text\"].fillna('')\n",
        "    content = content.dropna()\n",
        "    content_combined = content[\"language\"] + \" \" + content[\"title\"] + \" \" + content[\"description\"] + \" \" + content[\"text\"]\n",
        "    content_combined = pd.DataFrame({\"id\":content[\"id\"], \"features\":content_combined})\n",
        "    print(\"content_combined\", content_combined.shape)\n",
        "\n",
        "    topics[\"description\"] = topics[\"description\"].fillna('')\n",
        "    topics = topics.dropna()\n",
        "    topics_combined = topics[\"language\"] + \" \" + topics[\"channel\"] + ' ' + topics[\"title\"] + \" \" + topics[\"description\"]\n",
        "    topics_combined = pd.DataFrame({\"id\":topics[\"id\"], \"features\":topics_combined})\n",
        "    print(\"topics_combined\", topics_combined.shape)\n",
        "    \n",
        "    #Explode correlations rows\n",
        "    correlations[\"content_ids\"] = correlations[\"content_ids\"].str.split()\n",
        "    correlations = correlations.explode(\"content_ids\")\n",
        "\n",
        "    #Merge\n",
        "    merged = correlations.merge(topics_combined, how=\"inner\", left_on=\"topic_id\", right_on=\"id\")\n",
        "    print(\"merged\", merged.shape)\n",
        "    merged = merged.reset_index().merge(content_combined, how=\"inner\", left_on=\"content_ids\", right_on=\"id\", sort=False, suffixes=(\"_topics\", \"_content\")).sort_values(axis=0, by=\"index\")\n",
        "    merged = merged.drop([\"content_ids\", \"topic_id\"], axis=1)\n",
        "    print(\"merged\", merged.shape)\n",
        "\n",
        "    #Split\n",
        "    corr_topics = merged[['index', 'features_topics']]\n",
        "    corr_topics.columns = ['id', 'features']\n",
        "    corr_content = merged[['index', 'features_content']]\n",
        "    corr_content.columns = ['id', 'features']\n",
        "\n",
        "    index_to_topic = pd.Series(merged.id_topics.values, index=merged.index).to_dict()\n",
        "    index_to_content = pd.Series(merged.id_content.values, index=merged.index).to_dict()\n",
        "\n",
        "    return corr_topics, corr_content, index_to_topic, index_to_content"
      ],
      "metadata": {
        "id": "p0zJcO-_LU1a"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "#### Apply combine() to our data\n",
        "corr_topics, corr_content, index_to_topic, index_to_content = combine(correlations, topics, content)\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Atm-OtRKLYQB",
        "outputId": "f8c0185a-c3d0-4099-9a0a-a42c62ba317d"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "content_combined (16906, 2)\n",
            "topics_combined (76799, 2)\n",
            "merged (279892, 4)\n",
            "merged (47435, 5)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#### Create a stopword removal function to remove stopwords for each language\n",
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "# Dictionary of languages found in our data\n",
        "lang_dict = {\n",
        "    \"en\":\"english\",\n",
        "    \"es\":\"spanish\",\n",
        "    \"it\":\"italian\",\n",
        "    'pt':\"portuguese\",\n",
        "    'mr':'marathi',\n",
        "    'bg':'bulgarian',\n",
        "    'gu':'gujarati',\n",
        "    'sw':'swahili',\n",
        "    'hi':'hindi',\n",
        "    'ar':'arabic',\n",
        "    'bn':'bengali',\n",
        "    'as':'assamese',\n",
        "    'zh':'chinese',\n",
        "    'fr':'french',\n",
        "    'km':'khmer',\n",
        "    'pl':'polish',\n",
        "    'ta':'tamil',\n",
        "    'or':'oriya',\n",
        "    'ru':'russian',\n",
        "    'kn':'kannada',\n",
        "    'swa':'swahili',\n",
        "    'my':'burmese',\n",
        "    'pnb':'punjabi',\n",
        "    'fil':'filipino',\n",
        "    'tr':'turkish',\n",
        "    'te':'telugu',\n",
        "    'ur':'urdu',\n",
        "    'fi':'finnish',\n",
        "    'pn':'unknown',\n",
        "    'mu':'unknown'}\n",
        "\n",
        "# List of languages supported by the natural language tool kit (NLTK) module.\n",
        "supported_languages = stopwords.fileids()\n",
        "\n",
        "def remove_stopwords(text):\n",
        "    '''\n",
        "    Checks language of text then removes stopwords from that language if supported.\n",
        "    '''\n",
        "    lang_code = text[0:2]\n",
        "    if lang_dict[lang_code] in supported_languages:\n",
        "        for word in stopwords.words(lang_dict[lang_code]):\n",
        "            text = text.replace(' ' + word + ' ', ' ')\n",
        "    return text"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4KghXEypLaqK",
        "outputId": "9ff863bc-7065-4c6d-8715-4fefa90cb10e"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#### Apply remove_stopwords() to our data\n",
        "corr_topics[\"features\"] = corr_topics.features.apply(remove_stopwords)\n",
        "corr_content[\"features\"] = corr_content.features.apply(remove_stopwords)"
      ],
      "metadata": {
        "id": "H55Z_xzzLdK_"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#### Create train/test indices for our data \n",
        "random.seed(10)\n",
        "train_indices = random.sample(range(len(corr_content)), round(0.8*len(corr_content))) #80/20 train/test split\n",
        "\n",
        "#### Split training data so 50% is matching and 50% is not matching\n",
        "half = round(len(train_indices) / 2)\n",
        "full = len(train_indices)\n",
        "\n",
        "train_topics_half = corr_topics.iloc[train_indices[:half], :]\n",
        "train_content_half = corr_content.iloc[train_indices[:half], :]\n",
        "\n",
        "#Shift second half so that topics/content are not matching\n",
        "train_topics_full = corr_topics.iloc[train_indices[half:(full-20)], :] \n",
        "train_content_full = corr_content.iloc[train_indices[(half+20):(full)], :] \n",
        "\n",
        "train_topics = pd.concat([train_topics_half, train_topics_full]).reset_index().drop(\"index\", axis=1)\n",
        "train_content = pd.concat([train_content_half, train_content_full]).reset_index().drop(\"index\", axis=1)"
      ],
      "metadata": {
        "id": "GEVFT2uALvu1"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#### Repeat for test data\n",
        "test_topics = corr_topics.drop(train_indices, axis=0)\n",
        "test_content = corr_content.drop(train_indices, axis=0)\n",
        "\n",
        "half = round(len(test_topics.features) / 2)\n",
        "full = len(test_topics.features)\n",
        "\n",
        "test_topics_half = test_topics.iloc[:half, :]\n",
        "test_content_half = test_content.iloc[:half, :]\n",
        "\n",
        "test_topics_full = test_topics.iloc[half:(full - 5), :]\n",
        "test_content_full = test_content.iloc[(half+5):full, :]\n",
        "\n",
        "test_topics = pd.concat([test_topics_half, test_topics_full]).reset_index().drop(\"index\", axis=1)\n",
        "test_content = pd.concat([test_content_half, test_content_full]).reset_index().drop(\"index\", axis=1)"
      ],
      "metadata": {
        "id": "Rb1I1pTNLyV-"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#### Create labels\n",
        "train_labels = np.array((train_topics.id == train_content.id).astype(int))\n",
        "test_labels = np.array((test_topics.id == test_content.id).astype(int))"
      ],
      "metadata": {
        "id": "breawo_9Lz6x"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#### Convert data to tensors\n",
        "train_topics = tf.data.Dataset.from_tensor_slices(tf.cast(train_topics.features, tf.string))\n",
        "train_content = tf.data.Dataset.from_tensor_slices(tf.cast(train_content.features, tf.string))\n",
        "train_labels = tf.data.Dataset.from_tensor_slices(tf.cast(train_labels, tf.int32))\n",
        "\n",
        "test_topics = tf.data.Dataset.from_tensor_slices(tf.cast(test_topics.features, tf.string))\n",
        "test_content = tf.data.Dataset.from_tensor_slices(tf.cast(test_content.features, tf.string))\n",
        "test_labels = tf.data.Dataset.from_tensor_slices(tf.cast(test_labels, tf.int32))"
      ],
      "metadata": {
        "id": "GsMMgzQ3L2XV"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#### Combine data into TensorFlow Datasets\n",
        "#### Perfectly shuffle, batch, cache, and prefetch our new datasets\n",
        "train_ds = tf.data.Dataset.zip(\n",
        "    ((train_topics, train_content), train_labels)\n",
        ")\n",
        "\n",
        "train_ds = train_ds.shuffle(buffer_size = train_ds.cardinality().numpy()).batch(batch_size = 64).cache().prefetch(tf.data.experimental.AUTOTUNE)\n",
        "\n",
        "test_ds = tf.data.Dataset.zip(\n",
        "    ((test_topics, test_content), test_labels)\n",
        ")\n",
        "\n",
        "test_ds = test_ds.shuffle(buffer_size = test_ds.cardinality().numpy()).batch(batch_size = 64).cache().prefetch(tf.data.experimental.AUTOTUNE)"
      ],
      "metadata": {
        "id": "JPLN-m0oL3rH"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#### Create Text Vectorization Layer\n",
        "# Hyperparameters\n",
        "VOCAB_SIZE = 1000000\n",
        "MAX_LEN = 50\n",
        "\n",
        "def my_standardize(text): \n",
        "    '''\n",
        "    A text standardization function that is applied for every element in the vectorize layer. \n",
        "    '''\n",
        "    text = tf.strings.lower(text, encoding='utf-8') #lowercase\n",
        "    text = tf.strings.regex_replace(text, f\"([{string.punctuation}])\", r\" \") #remove punctuation\n",
        "    text = tf.strings.regex_replace(text, '\\n', \"\") #remove newlines\n",
        "    text = tf.strings.regex_replace(text, ' +', \" \") #remove 2+ whitespaces\n",
        "    text = tf.strings.strip(text) #remove leading and tailing whitespaces\n",
        "    return text\n",
        "\n",
        "vectorize_layer = TextVectorization(\n",
        "    standardize = my_standardize,\n",
        "    split = \"whitespace\",\n",
        "    max_tokens = VOCAB_SIZE + 2,\n",
        "    output_mode = 'int',\n",
        "    output_sequence_length = MAX_LEN\n",
        ")"
      ],
      "metadata": {
        "id": "nZmh8LUxL6Ci"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#### Adapt text vectorization layer to our data\n",
        "vectorize_layer.adapt(pd.concat([corr_topics[\"features\"], corr_content[\"features\"]]))"
      ],
      "metadata": {
        "id": "egV0jjhJMCuc"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "inp_topics = Input((1, ), dtype=tf.string)\n",
        "inp_content = Input((1, ), dtype=tf.string)\n",
        "\n",
        "vectorized_topics = vectorize_layer(inp_topics)\n",
        "vectorized_content = vectorize_layer(inp_content)\n",
        "\n",
        "snn = Sequential([ \n",
        "  Embedding(VOCAB_SIZE, 256),\n",
        "  GlobalAveragePooling1D(),\n",
        "  Flatten(),\n",
        "  Dense(128, activation='relu'),\n",
        "])\n",
        "\n",
        "snn_content = snn(vectorized_content)\n",
        "snn_topics = snn(vectorized_topics)\n",
        "\n",
        "concat = Concatenate()([snn_topics, snn_content])\n",
        "\n",
        "dense = Dense(64, activation='relu')(concat)\n",
        "\n",
        "output = Dense(1, activation='sigmoid')(dense)\n",
        "\n",
        "model = Model(inputs=[inp_topics, inp_content], outputs=output)\n",
        "\n",
        "model.summary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4KWrcJdAMEj4",
        "outputId": "11de4bc7-2493-4a04-b0af-f9d3f71f8e8a"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"model\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                   Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            " input_1 (InputLayer)           [(None, 1)]          0           []                               \n",
            "                                                                                                  \n",
            " input_2 (InputLayer)           [(None, 1)]          0           []                               \n",
            "                                                                                                  \n",
            " text_vectorization (TextVector  (None, 50)          0           ['input_1[0][0]',                \n",
            " ization)                                                         'input_2[0][0]']                \n",
            "                                                                                                  \n",
            " sequential (Sequential)        (None, 128)          256032896   ['text_vectorization[1][0]',     \n",
            "                                                                  'text_vectorization[0][0]']     \n",
            "                                                                                                  \n",
            " concatenate (Concatenate)      (None, 256)          0           ['sequential[1][0]',             \n",
            "                                                                  'sequential[0][0]']             \n",
            "                                                                                                  \n",
            " dense_1 (Dense)                (None, 64)           16448       ['concatenate[0][0]']            \n",
            "                                                                                                  \n",
            " dense_2 (Dense)                (None, 1)            65          ['dense_1[0][0]']                \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 256,049,409\n",
            "Trainable params: 256,049,409\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.compile(loss='binary_crossentropy',\n",
        "              optimizer='adam',\n",
        "              metrics=tf.keras.metrics.AUC())"
      ],
      "metadata": {
        "id": "xBoGYP_RMHFz"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.fit(train_ds, epochs=5, verbose=1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XgIbcNk8MKwf",
        "outputId": "43500c5d-0904-4f34-f8f0-9869030d4561"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/5\n",
            "593/593 [==============================] - 119s 190ms/step - loss: 0.6067 - auc: 0.7002\n",
            "Epoch 2/5\n",
            "204/593 [=========>....................] - ETA: 34s - loss: 0.4760 - auc: 0.8382"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.evaluate(test_ds, verbose=1)"
      ],
      "metadata": {
        "id": "bFh3K_bqMOIl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#### Antijoin topics with correlations data, since we don't have to predict those topics\n",
        "outer_joined = topics.merge(correlations, how='outer', left_on='id', right_on='topic_id', indicator=True)\n",
        "topics = outer_joined[(outer_joined._merge == 'left_only')].drop('_merge', axis=1)\n",
        "\n",
        "#### Fill missing values and concatenate text to features column \n",
        "topics = topics.fillna(\"\")\n",
        "topics_ids = topics.id.values\n",
        "topics_lang = topics.language\n",
        "topics_index = topics.index\n",
        "topics_features = topics[\"language\"] + ' ' + topics[\"channel\"] + ' ' + topics[\"title\"] + ' ' + topics[\"description\"]\n",
        "del topics\n",
        "\n",
        "#### Repeat for content, except we keep all content data\n",
        "content = content.fillna(\"\")\n",
        "content_ids = content.id.values\n",
        "content_index = content.index\n",
        "content_lang = content.language\n",
        "content_features = content[\"language\"] + ' ' + content[\"title\"] + ' ' + content[\"description\"] + ' ' + content[\"text\"]\n",
        "del content\n",
        "\n",
        "index_to_content = pd.Series(content_ids, index=content_index).to_dict()\n",
        "index_to_topic = pd.Series(topics_ids, index=topics_index).to_dict()"
      ],
      "metadata": {
        "id": "JEvE7OAaMOwP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#### Remove stopwords\n",
        "topics_features = topics_features.apply(remove_stopwords)\n",
        "content_features = content_features.apply(remove_stopwords)"
      ],
      "metadata": {
        "id": "6R6ri9SvMRXR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#### Write predictions to output_file\n",
        "THRESHOLD = 0.99994\n",
        "\n",
        "output_file = \"submission.csv\"\n",
        "f = open(output_file, 'w')\n",
        "\n",
        "writer = csv.writer(f)\n",
        "writer.writerow([\"topic_id\", \"content_ids\"])\n",
        "\n",
        "for i in topics_features.index:\n",
        "    temp_content = tf.data.Dataset.from_tensor_slices(\n",
        "        tf.cast(content_features[content_lang == topics_lang[i]], tf.string)\n",
        "    )\n",
        "    temp_topic = tf.data.Dataset.from_tensor_slices(\n",
        "        tf.cast(np.repeat(topics_features[i], len(temp_content)), tf.string)\n",
        "    )\n",
        "    temp_ds = tf.data.Dataset.zip(((temp_topic, temp_content), ))\\\n",
        "        .batch(batch_size=64)\\\n",
        "            .cache()\\\n",
        "                .prefetch(tf.data.experimental.AUTOTUNE)\n",
        "    matches = model.predict(temp_ds, verbose=0)\n",
        "    matches = [i for i in range(len(matches)) if matches[i] > THRESHOLD]\n",
        "    matches = \" \".join([index_to_content[x] for x in matches])\n",
        "    writer.writerow([index_to_topic[i], matches])\n",
        "\n",
        "#### Add given correlations data\n",
        "writer.writerows([correlations.topic_id, correlations.content_ids])    \n",
        "\n",
        "f.close()"
      ],
      "metadata": {
        "id": "7NuFiXvFMVwG"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}