{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "2Tr6AjrnKsbB"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-04-24 22:29:31.780105: I tensorflow/core/util/port.cc:110] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2023-04-24 22:29:31.970132: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-04-24 22:29:32.826438: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "#### Import necessary packages/functions\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import *\n",
    "from tensorflow.keras.models import Sequential, Model\n",
    "from nltk.corpus import stopwords\n",
    "import string\n",
    "from unidecode import unidecode\n",
    "import random\n",
    "import itertools\n",
    "import csv\n",
    "from tqdm import tqdm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "qQHJE9USKuxR"
   },
   "outputs": [],
   "source": [
    "PATH = \"../data/\"\n",
    "\n",
    "#### Reading in the data.\n",
    "content = pd.read_csv(f\"{PATH}content_filtered.csv\")\n",
    "correlations = pd.read_csv(f\"{PATH}correlations.csv\")\n",
    "#sample_submission = pd.read_csv(\"/kaggle/input/learning-equality-curriculum-recommendations/sample_submission.csv\")\n",
    "topics = pd.read_csv(f\"{PATH}topics_filtered.csv\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "p0zJcO-_LU1a"
   },
   "outputs": [],
   "source": [
    "#### Create combine function\n",
    "def combine(correlations, topics, content):\n",
    "    '''\n",
    "    - Inputs our three datasets and combines the topic/content information with the topic/content correlations data. \n",
    "    - All topic/content information is concatenated to one \"features\" column, which includes the language, title, description, etc.\n",
    "    - Output includes the correlations topics information, correlations content information, and a dictionary to convert indices to their\n",
    "      corresponding topic/content id. \n",
    "    '''\n",
    "    #Drop/combine columns\n",
    "    content[\"text\"] = content[\"text\"].fillna('')\n",
    "    content = content.dropna()\n",
    "    content_combined = content[\"language\"] + \" \" + content[\"title\"] + \" \" + content[\"description\"] + \" \" + content[\"text\"]\n",
    "    content_combined = pd.DataFrame({\"id\":content[\"id\"], \"features\":content_combined})\n",
    "    print(\"content_combined\", content_combined.shape)\n",
    "\n",
    "    topics[\"description\"] = topics[\"description\"].fillna('')\n",
    "    topics = topics.dropna()\n",
    "    topics_combined = topics[\"language\"] + \" \" + topics[\"channel\"] + ' ' + topics[\"title\"] + \" \" + topics[\"description\"]\n",
    "    topics_combined = pd.DataFrame({\"id\":topics[\"id\"], \"features\":topics_combined})\n",
    "    print(\"topics_combined\", topics_combined.shape)\n",
    "    \n",
    "    #Explode correlations rows\n",
    "    correlations[\"content_ids\"] = correlations[\"content_ids\"].str.split()\n",
    "    correlations = correlations.explode(\"content_ids\")\n",
    "\n",
    "    #Merge\n",
    "    merged = correlations.merge(topics_combined, how=\"inner\", left_on=\"topic_id\", right_on=\"id\")\n",
    "    print(\"merged\", merged.shape)\n",
    "    merged = merged.reset_index().merge(content_combined, how=\"inner\", left_on=\"content_ids\", right_on=\"id\", sort=False, suffixes=(\"_topics\", \"_content\")).sort_values(axis=0, by=\"index\")\n",
    "    merged = merged.drop([\"content_ids\", \"topic_id\"], axis=1)\n",
    "    print(\"merged\", merged.shape)\n",
    "\n",
    "    #Split\n",
    "    corr_topics = merged[['index', 'features_topics']]\n",
    "    corr_topics.columns = ['id', 'features']\n",
    "    corr_content = merged[['index', 'features_content']]\n",
    "    corr_content.columns = ['id', 'features']\n",
    "\n",
    "    index_to_topic = pd.Series(merged.id_topics.values, index=merged.index).to_dict()\n",
    "    index_to_content = pd.Series(merged.id_content.values, index=merged.index).to_dict()\n",
    "\n",
    "    return corr_topics, corr_content, index_to_topic, index_to_content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Atm-OtRKLYQB",
    "outputId": "f8c0185a-c3d0-4099-9a0a-a42c62ba317d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "content_combined (16906, 2)\n",
      "topics_combined (36078, 2)\n",
      "merged (127725, 4)\n",
      "merged (47435, 5)\n"
     ]
    }
   ],
   "source": [
    "#### Apply combine() to our data\n",
    "corr_topics, corr_content, index_to_topic, index_to_content = combine(correlations, topics, content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "4KghXEypLaqK",
    "outputId": "9ff863bc-7065-4c6d-8715-4fefa90cb10e"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /home/ghufran/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "#### Create a stopword removal function to remove stopwords for each language\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "# Dictionary of languages found in our data\n",
    "lang_dict = {\n",
    "    \"en\":\"english\",\n",
    "    \"es\":\"spanish\",\n",
    "    \"it\":\"italian\",\n",
    "    'pt':\"portuguese\",\n",
    "    'mr':'marathi',\n",
    "    'bg':'bulgarian',\n",
    "    'gu':'gujarati',\n",
    "    'sw':'swahili',\n",
    "    'hi':'hindi',\n",
    "    'ar':'arabic',\n",
    "    'bn':'bengali',\n",
    "    'as':'assamese',\n",
    "    'zh':'chinese',\n",
    "    'fr':'french',\n",
    "    'km':'khmer',\n",
    "    'pl':'polish',\n",
    "    'ta':'tamil',\n",
    "    'or':'oriya',\n",
    "    'ru':'russian',\n",
    "    'kn':'kannada',\n",
    "    'swa':'swahili',\n",
    "    'my':'burmese',\n",
    "    'pnb':'punjabi',\n",
    "    'fil':'filipino',\n",
    "    'tr':'turkish',\n",
    "    'te':'telugu',\n",
    "    'ur':'urdu',\n",
    "    'fi':'finnish',\n",
    "    'pn':'unknown',\n",
    "    'mu':'unknown'}\n",
    "\n",
    "# List of languages supported by the natural language tool kit (NLTK) module.\n",
    "supported_languages = stopwords.fileids()\n",
    "\n",
    "def remove_stopwords(text):\n",
    "    '''\n",
    "    Checks language of text then removes stopwords from that language if supported.\n",
    "    '''\n",
    "    lang_code = text[0:2]\n",
    "    if lang_dict[lang_code] in supported_languages:\n",
    "        for word in stopwords.words(lang_dict[lang_code]):\n",
    "            text = text.replace(' ' + word + ' ', ' ')\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "H55Z_xzzLdK_"
   },
   "outputs": [],
   "source": [
    "#### Apply remove_stopwords() to our data\n",
    "corr_topics[\"features\"] = corr_topics.features.apply(remove_stopwords)\n",
    "corr_content[\"features\"] = corr_content.features.apply(remove_stopwords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "GEVFT2uALvu1"
   },
   "outputs": [],
   "source": [
    "#### Create train/test indices for our data \n",
    "random.seed(10)\n",
    "train_indices = random.sample(range(len(corr_content)), round(0.8*len(corr_content))) #80/20 train/test split\n",
    "\n",
    "#### Split training data so 50% is matching and 50% is not matching\n",
    "half = round(len(train_indices) / 2)\n",
    "full = len(train_indices)\n",
    "\n",
    "train_topics_half = corr_topics.iloc[train_indices[:half], :]\n",
    "train_content_half = corr_content.iloc[train_indices[:half], :]\n",
    "\n",
    "#Shift second half so that topics/content are not matching\n",
    "train_topics_full = corr_topics.iloc[train_indices[half:(full-20)], :] \n",
    "train_content_full = corr_content.iloc[train_indices[(half+20):(full)], :] \n",
    "\n",
    "train_topics = pd.concat([train_topics_half, train_topics_full]).reset_index().drop(\"index\", axis=1)\n",
    "train_content = pd.concat([train_content_half, train_content_full]).reset_index().drop(\"index\", axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "Rb1I1pTNLyV-"
   },
   "outputs": [],
   "source": [
    "#### Repeat for test data\n",
    "test_topics = corr_topics.drop(train_indices, axis=0)\n",
    "test_content = corr_content.drop(train_indices, axis=0)\n",
    "\n",
    "half = round(len(test_topics.features) / 2)\n",
    "full = len(test_topics.features)\n",
    "\n",
    "test_topics_half = test_topics.iloc[:half, :]\n",
    "test_content_half = test_content.iloc[:half, :]\n",
    "\n",
    "test_topics_full = test_topics.iloc[half:(full - 5), :]\n",
    "test_content_full = test_content.iloc[(half+5):full, :]\n",
    "\n",
    "test_topics = pd.concat([test_topics_half, test_topics_full]).reset_index().drop(\"index\", axis=1)\n",
    "test_content = pd.concat([test_content_half, test_content_full]).reset_index().drop(\"index\", axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "breawo_9Lz6x"
   },
   "outputs": [],
   "source": [
    "#### Create labels\n",
    "train_labels = np.array((train_topics.id == train_content.id).astype(int))\n",
    "test_labels = np.array((test_topics.id == test_content.id).astype(int))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "GsMMgzQ3L2XV"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-04-24 22:30:40.448887: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:982] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2023-04-24 22:30:40.518687: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:982] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2023-04-24 22:30:40.519084: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:982] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2023-04-24 22:30:40.523263: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:982] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2023-04-24 22:30:40.523519: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:982] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2023-04-24 22:30:40.523703: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:982] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2023-04-24 22:30:41.628071: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:982] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2023-04-24 22:30:41.628716: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:982] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2023-04-24 22:30:41.628747: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1722] Could not identify NUMA node of platform GPU id 0, defaulting to 0.  Your kernel may not have been built with NUMA support.\n",
      "2023-04-24 22:30:41.629227: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:982] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2023-04-24 22:30:41.629266: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1635] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 5362 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 3070 Ti Laptop GPU, pci bus id: 0000:01:00.0, compute capability: 8.6\n"
     ]
    }
   ],
   "source": [
    "#### Convert data to tensors\n",
    "train_topics = tf.data.Dataset.from_tensor_slices(tf.cast(train_topics.features, tf.string))\n",
    "train_content = tf.data.Dataset.from_tensor_slices(tf.cast(train_content.features, tf.string))\n",
    "train_labels = tf.data.Dataset.from_tensor_slices(tf.cast(train_labels, tf.int32))\n",
    "\n",
    "test_topics = tf.data.Dataset.from_tensor_slices(tf.cast(test_topics.features, tf.string))\n",
    "test_content = tf.data.Dataset.from_tensor_slices(tf.cast(test_content.features, tf.string))\n",
    "test_labels = tf.data.Dataset.from_tensor_slices(tf.cast(test_labels, tf.int32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "JPLN-m0oL3rH"
   },
   "outputs": [],
   "source": [
    "#### Combine data into TensorFlow Datasets\n",
    "#### Perfectly shuffle, batch, cache, and prefetch our new datasets\n",
    "train_ds = tf.data.Dataset.zip(\n",
    "    ((train_topics, train_content), train_labels)\n",
    ")\n",
    "\n",
    "train_ds = train_ds.shuffle(buffer_size = train_ds.cardinality().numpy()).batch(batch_size = 64).cache().prefetch(tf.data.experimental.AUTOTUNE)\n",
    "\n",
    "test_ds = tf.data.Dataset.zip(\n",
    "    ((test_topics, test_content), test_labels)\n",
    ")\n",
    "\n",
    "test_ds = test_ds.shuffle(buffer_size = test_ds.cardinality().numpy()).batch(batch_size = 64).cache().prefetch(tf.data.experimental.AUTOTUNE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "nZmh8LUxL6Ci"
   },
   "outputs": [],
   "source": [
    "#### Create Text Vectorization Layer\n",
    "# Hyperparameters\n",
    "VOCAB_SIZE = 1000000\n",
    "MAX_LEN = 50\n",
    "\n",
    "def my_standardize(text): \n",
    "    '''\n",
    "    A text standardization function that is applied for every element in the vectorize layer. \n",
    "    '''\n",
    "    text = tf.strings.lower(text, encoding='utf-8') #lowercase\n",
    "    text = tf.strings.regex_replace(text, f\"([{string.punctuation}])\", r\" \") #remove punctuation\n",
    "    text = tf.strings.regex_replace(text, '\\n', \"\") #remove newlines\n",
    "    text = tf.strings.regex_replace(text, ' +', \" \") #remove 2+ whitespaces\n",
    "    text = tf.strings.strip(text) #remove leading and tailing whitespaces\n",
    "    return text\n",
    "\n",
    "vectorize_layer = TextVectorization(\n",
    "    standardize = my_standardize,\n",
    "    split = \"whitespace\",\n",
    "    max_tokens = VOCAB_SIZE + 2,\n",
    "    output_mode = 'int',\n",
    "    output_sequence_length = MAX_LEN\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "egV0jjhJMCuc"
   },
   "outputs": [],
   "source": [
    "#### Adapt text vectorization layer to our data\n",
    "vectorize_layer.adapt(pd.concat([corr_topics[\"features\"], corr_content[\"features\"]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "4KWrcJdAMEj4",
    "outputId": "11de4bc7-2493-4a04-b0af-f9d3f71f8e8a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_1 (InputLayer)           [(None, 1)]          0           []                               \n",
      "                                                                                                  \n",
      " input_2 (InputLayer)           [(None, 1)]          0           []                               \n",
      "                                                                                                  \n",
      " text_vectorization (TextVector  (None, 50)          0           ['input_1[0][0]',                \n",
      " ization)                                                         'input_2[0][0]']                \n",
      "                                                                                                  \n",
      " sequential (Sequential)        (None, 128)          256032896   ['text_vectorization[1][0]',     \n",
      "                                                                  'text_vectorization[0][0]']     \n",
      "                                                                                                  \n",
      " concatenate (Concatenate)      (None, 256)          0           ['sequential[1][0]',             \n",
      "                                                                  'sequential[0][0]']             \n",
      "                                                                                                  \n",
      " dense_1 (Dense)                (None, 64)           16448       ['concatenate[0][0]']            \n",
      "                                                                                                  \n",
      " dense_2 (Dense)                (None, 1)            65          ['dense_1[0][0]']                \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 256,049,409\n",
      "Trainable params: 256,049,409\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "inp_topics = Input((1, ), dtype=tf.string)\n",
    "inp_content = Input((1, ), dtype=tf.string)\n",
    "\n",
    "vectorized_topics = vectorize_layer(inp_topics)\n",
    "vectorized_content = vectorize_layer(inp_content)\n",
    "\n",
    "snn = Sequential([ \n",
    "  Embedding(VOCAB_SIZE, 256),\n",
    "  GlobalAveragePooling1D(),\n",
    "  Flatten(),\n",
    "  Dense(128, activation='relu'),\n",
    "])\n",
    "\n",
    "snn_content = snn(vectorized_content)\n",
    "snn_topics = snn(vectorized_topics)\n",
    "\n",
    "concat = Concatenate()([snn_topics, snn_content])\n",
    "\n",
    "dense = Dense(64, activation='relu')(concat)\n",
    "\n",
    "output = Dense(1, activation='sigmoid')(dense)\n",
    "\n",
    "model = Model(inputs=[inp_topics, inp_content], outputs=output)\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "xBoGYP_RMHFz"
   },
   "outputs": [],
   "source": [
    "model.compile(loss='binary_crossentropy',\n",
    "              optimizer='adam',\n",
    "              metrics=tf.keras.metrics.AUC())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "XgIbcNk8MKwf",
    "outputId": "43500c5d-0904-4f34-f8f0-9869030d4561"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-04-24 22:30:57.207559: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'Placeholder/_0' with dtype string and shape [37928]\n",
      "\t [[{{node Placeholder/_0}}]]\n",
      "2023-04-24 22:30:57.207790: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'Placeholder/_0' with dtype string and shape [37928]\n",
      "\t [[{{node Placeholder/_0}}]]\n",
      "2023-04-24 22:30:58.820012: I tensorflow/compiler/xla/stream_executor/cuda/cuda_blas.cc:637] TensorFloat-32 will be used for the matrix multiplication. This will only be logged once.\n",
      "2023-04-24 22:30:58.843450: I tensorflow/compiler/xla/service/service.cc:169] XLA service 0x7f0aec029e40 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
      "2023-04-24 22:30:58.843484: I tensorflow/compiler/xla/service/service.cc:177]   StreamExecutor device (0): NVIDIA GeForce RTX 3070 Ti Laptop GPU, Compute Capability 8.6\n",
      "2023-04-24 22:30:58.866051: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:269] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\n",
      "2023-04-24 22:31:01.071572: I tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:424] Loaded cuDNN version 8900\n",
      "2023-04-24 22:31:01.319376: I tensorflow/tsl/platform/default/subprocess.cc:304] Start cannot spawn child process: No such file or directory\n",
      "2023-04-24 22:31:01.428727: I ./tensorflow/compiler/jit/device_compiler.h:180] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n",
      "2023-04-24 22:31:12.048918: W tensorflow/tsl/framework/bfc_allocator.cc:485] Allocator (GPU_0_bfc) ran out of memory trying to allocate 1.91GiB (rounded to 2048000512)requested by op \n",
      "2023-04-24 22:31:12.048992: I tensorflow/tsl/framework/bfc_allocator.cc:1039] BFCAllocator dump for GPU_0_bfc\n",
      "2023-04-24 22:31:12.049005: I tensorflow/tsl/framework/bfc_allocator.cc:1046] Bin (256): \tTotal Chunks: 42, Chunks in use: 41. 10.5KiB allocated for chunks. 10.2KiB in use in bin. 1.7KiB client-requested in use in bin.\n",
      "2023-04-24 22:31:12.049010: I tensorflow/tsl/framework/bfc_allocator.cc:1046] Bin (512): \tTotal Chunks: 4, Chunks in use: 3. 2.0KiB allocated for chunks. 1.5KiB in use in bin. 1.5KiB client-requested in use in bin.\n",
      "2023-04-24 22:31:12.049014: I tensorflow/tsl/framework/bfc_allocator.cc:1046] Bin (1024): \tTotal Chunks: 6, Chunks in use: 5. 6.8KiB allocated for chunks. 5.2KiB in use in bin. 4.1KiB client-requested in use in bin.\n",
      "2023-04-24 22:31:12.049020: I tensorflow/tsl/framework/bfc_allocator.cc:1046] Bin (2048): \tTotal Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.\n",
      "2023-04-24 22:31:12.049024: I tensorflow/tsl/framework/bfc_allocator.cc:1046] Bin (4096): \tTotal Chunks: 1, Chunks in use: 1. 7.0KiB allocated for chunks. 7.0KiB in use in bin. 6.7KiB client-requested in use in bin.\n",
      "2023-04-24 22:31:12.049029: I tensorflow/tsl/framework/bfc_allocator.cc:1046] Bin (8192): \tTotal Chunks: 1, Chunks in use: 0. 14.0KiB allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.\n",
      "2023-04-24 22:31:12.049033: I tensorflow/tsl/framework/bfc_allocator.cc:1046] Bin (16384): \tTotal Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.\n",
      "2023-04-24 22:31:12.049038: I tensorflow/tsl/framework/bfc_allocator.cc:1046] Bin (32768): \tTotal Chunks: 2, Chunks in use: 1. 72.5KiB allocated for chunks. 37.2KiB in use in bin. 37.0KiB client-requested in use in bin.\n",
      "2023-04-24 22:31:12.049043: I tensorflow/tsl/framework/bfc_allocator.cc:1046] Bin (65536): \tTotal Chunks: 4, Chunks in use: 3. 363.0KiB allocated for chunks. 249.0KiB in use in bin. 192.0KiB client-requested in use in bin.\n",
      "2023-04-24 22:31:12.049048: I tensorflow/tsl/framework/bfc_allocator.cc:1046] Bin (131072): \tTotal Chunks: 4, Chunks in use: 4. 532.2KiB allocated for chunks. 532.2KiB in use in bin. 532.2KiB client-requested in use in bin.\n",
      "2023-04-24 22:31:12.049052: I tensorflow/tsl/framework/bfc_allocator.cc:1046] Bin (262144): \tTotal Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.\n",
      "2023-04-24 22:31:12.049056: I tensorflow/tsl/framework/bfc_allocator.cc:1046] Bin (524288): \tTotal Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.\n",
      "2023-04-24 22:31:12.049060: I tensorflow/tsl/framework/bfc_allocator.cc:1046] Bin (1048576): \tTotal Chunks: 1, Chunks in use: 1. 1.68MiB allocated for chunks. 1.68MiB in use in bin. 1.68MiB client-requested in use in bin.\n",
      "2023-04-24 22:31:12.049065: I tensorflow/tsl/framework/bfc_allocator.cc:1046] Bin (2097152): \tTotal Chunks: 1, Chunks in use: 1. 3.11MiB allocated for chunks. 3.11MiB in use in bin. 3.11MiB client-requested in use in bin.\n",
      "2023-04-24 22:31:12.049069: I tensorflow/tsl/framework/bfc_allocator.cc:1046] Bin (4194304): \tTotal Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.\n",
      "2023-04-24 22:31:12.049072: I tensorflow/tsl/framework/bfc_allocator.cc:1046] Bin (8388608): \tTotal Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.\n",
      "2023-04-24 22:31:12.049077: I tensorflow/tsl/framework/bfc_allocator.cc:1046] Bin (16777216): \tTotal Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.\n",
      "2023-04-24 22:31:12.049081: I tensorflow/tsl/framework/bfc_allocator.cc:1046] Bin (33554432): \tTotal Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.\n",
      "2023-04-24 22:31:12.049087: I tensorflow/tsl/framework/bfc_allocator.cc:1046] Bin (67108864): \tTotal Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.\n",
      "2023-04-24 22:31:12.049091: I tensorflow/tsl/framework/bfc_allocator.cc:1046] Bin (134217728): \tTotal Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.\n",
      "2023-04-24 22:31:12.049095: I tensorflow/tsl/framework/bfc_allocator.cc:1046] Bin (268435456): \tTotal Chunks: 5, Chunks in use: 3. 5.23GiB allocated for chunks. 2.86GiB in use in bin. 2.86GiB client-requested in use in bin.\n",
      "2023-04-24 22:31:12.049100: I tensorflow/tsl/framework/bfc_allocator.cc:1062] Bin for 1.91GiB was 256.00MiB, Chunk State: \n",
      "2023-04-24 22:31:12.049110: I tensorflow/tsl/framework/bfc_allocator.cc:1068]   Size: 974.34MiB | Requested Size: 3.12MiB | in_use: 0 | bin_num: 20, prev:   Size: 1.68MiB | Requested Size: 1.68MiB | in_use: 1 | bin_num: -1, next:   Size: 976.56MiB | Requested Size: 976.56MiB | in_use: 1 | bin_num: -1\n",
      "2023-04-24 22:31:12.049113: I tensorflow/tsl/framework/bfc_allocator.cc:1068]   Size: 1.42GiB | Requested Size: 0B | in_use: 0 | bin_num: 20, prev:   Size: 976.56MiB | Requested Size: 976.56MiB | in_use: 1 | bin_num: -1\n",
      "2023-04-24 22:31:12.049114: I tensorflow/tsl/framework/bfc_allocator.cc:1075] Next region of size 5622464512\n",
      "2023-04-24 22:31:12.049118: I tensorflow/tsl/framework/bfc_allocator.cc:1095] InUse at 716a00000 of size 1280 next 1\n",
      "2023-04-24 22:31:12.049120: I tensorflow/tsl/framework/bfc_allocator.cc:1095] InUse at 716a00500 of size 256 next 6\n",
      "2023-04-24 22:31:12.049121: I tensorflow/tsl/framework/bfc_allocator.cc:1095] InUse at 716a00600 of size 256 next 7\n",
      "2023-04-24 22:31:12.049123: I tensorflow/tsl/framework/bfc_allocator.cc:1095] InUse at 716a00700 of size 256 next 8\n",
      "2023-04-24 22:31:12.049125: I tensorflow/tsl/framework/bfc_allocator.cc:1095] InUse at 716a00800 of size 256 next 9\n",
      "2023-04-24 22:31:12.049126: I tensorflow/tsl/framework/bfc_allocator.cc:1095] InUse at 716a00900 of size 256 next 10\n",
      "2023-04-24 22:31:12.049128: I tensorflow/tsl/framework/bfc_allocator.cc:1095] InUse at 716a00a00 of size 256 next 11\n",
      "2023-04-24 22:31:12.049130: I tensorflow/tsl/framework/bfc_allocator.cc:1095] InUse at 716a00b00 of size 256 next 12\n",
      "2023-04-24 22:31:12.049131: I tensorflow/tsl/framework/bfc_allocator.cc:1095] InUse at 716a00c00 of size 256 next 13\n",
      "2023-04-24 22:31:12.049133: I tensorflow/tsl/framework/bfc_allocator.cc:1095] InUse at 716a00d00 of size 256 next 14\n",
      "2023-04-24 22:31:12.049134: I tensorflow/tsl/framework/bfc_allocator.cc:1095] InUse at 716a00e00 of size 256 next 15\n",
      "2023-04-24 22:31:12.049136: I tensorflow/tsl/framework/bfc_allocator.cc:1095] InUse at 716a00f00 of size 256 next 19\n",
      "2023-04-24 22:31:12.049138: I tensorflow/tsl/framework/bfc_allocator.cc:1095] InUse at 716a01000 of size 256 next 17\n",
      "2023-04-24 22:31:12.049139: I tensorflow/tsl/framework/bfc_allocator.cc:1095] InUse at 716a01100 of size 256 next 18\n",
      "2023-04-24 22:31:12.049141: I tensorflow/tsl/framework/bfc_allocator.cc:1095] InUse at 716a01200 of size 512 next 22\n",
      "2023-04-24 22:31:12.049143: I tensorflow/tsl/framework/bfc_allocator.cc:1095] InUse at 716a01400 of size 256 next 25\n",
      "2023-04-24 22:31:12.049144: I tensorflow/tsl/framework/bfc_allocator.cc:1095] InUse at 716a01500 of size 256 next 26\n",
      "2023-04-24 22:31:12.049146: I tensorflow/tsl/framework/bfc_allocator.cc:1095] InUse at 716a01600 of size 256 next 28\n",
      "2023-04-24 22:31:12.049148: I tensorflow/tsl/framework/bfc_allocator.cc:1095] InUse at 716a01700 of size 256 next 27\n",
      "2023-04-24 22:31:12.049149: I tensorflow/tsl/framework/bfc_allocator.cc:1095] InUse at 716a01800 of size 256 next 29\n",
      "2023-04-24 22:31:12.049151: I tensorflow/tsl/framework/bfc_allocator.cc:1095] InUse at 716a01900 of size 256 next 32\n",
      "2023-04-24 22:31:12.049152: I tensorflow/tsl/framework/bfc_allocator.cc:1095] InUse at 716a01a00 of size 256 next 38\n",
      "2023-04-24 22:31:12.049154: I tensorflow/tsl/framework/bfc_allocator.cc:1095] InUse at 716a01b00 of size 256 next 33\n",
      "2023-04-24 22:31:12.049156: I tensorflow/tsl/framework/bfc_allocator.cc:1095] InUse at 716a01c00 of size 256 next 34\n",
      "2023-04-24 22:31:12.049158: I tensorflow/tsl/framework/bfc_allocator.cc:1095] InUse at 716a01d00 of size 1024 next 31\n",
      "2023-04-24 22:31:12.049160: I tensorflow/tsl/framework/bfc_allocator.cc:1095] InUse at 716a02100 of size 1024 next 35\n",
      "2023-04-24 22:31:12.049161: I tensorflow/tsl/framework/bfc_allocator.cc:1095] InUse at 716a02500 of size 1024 next 36\n",
      "2023-04-24 22:31:12.049163: I tensorflow/tsl/framework/bfc_allocator.cc:1095] InUse at 716a02900 of size 1024 next 37\n",
      "2023-04-24 22:31:12.049165: I tensorflow/tsl/framework/bfc_allocator.cc:1095] InUse at 716a02d00 of size 256 next 39\n",
      "2023-04-24 22:31:12.049166: I tensorflow/tsl/framework/bfc_allocator.cc:1095] InUse at 716a02e00 of size 256 next 40\n",
      "2023-04-24 22:31:12.049168: I tensorflow/tsl/framework/bfc_allocator.cc:1095] InUse at 716a02f00 of size 512 next 44\n",
      "2023-04-24 22:31:12.049169: I tensorflow/tsl/framework/bfc_allocator.cc:1095] InUse at 716a03100 of size 512 next 45\n",
      "2023-04-24 22:31:12.049171: I tensorflow/tsl/framework/bfc_allocator.cc:1095] InUse at 716a03300 of size 256 next 47\n",
      "2023-04-24 22:31:12.049173: I tensorflow/tsl/framework/bfc_allocator.cc:1095] InUse at 716a03400 of size 256 next 48\n",
      "2023-04-24 22:31:12.049174: I tensorflow/tsl/framework/bfc_allocator.cc:1095] InUse at 716a03500 of size 256 next 49\n",
      "2023-04-24 22:31:12.049176: I tensorflow/tsl/framework/bfc_allocator.cc:1095] InUse at 716a03600 of size 256 next 50\n",
      "2023-04-24 22:31:12.049178: I tensorflow/tsl/framework/bfc_allocator.cc:1095] InUse at 716a03700 of size 256 next 51\n",
      "2023-04-24 22:31:12.049179: I tensorflow/tsl/framework/bfc_allocator.cc:1095] InUse at 716a03800 of size 256 next 52\n",
      "2023-04-24 22:31:12.049181: I tensorflow/tsl/framework/bfc_allocator.cc:1095] InUse at 716a03900 of size 256 next 53\n",
      "2023-04-24 22:31:12.049182: I tensorflow/tsl/framework/bfc_allocator.cc:1095] InUse at 716a03a00 of size 256 next 54\n",
      "2023-04-24 22:31:12.049184: I tensorflow/tsl/framework/bfc_allocator.cc:1095] InUse at 716a03b00 of size 256 next 55\n",
      "2023-04-24 22:31:12.049186: I tensorflow/tsl/framework/bfc_allocator.cc:1095] InUse at 716a03c00 of size 256 next 56\n",
      "2023-04-24 22:31:12.049187: I tensorflow/tsl/framework/bfc_allocator.cc:1095] InUse at 716a03d00 of size 256 next 57\n",
      "2023-04-24 22:31:12.049189: I tensorflow/tsl/framework/bfc_allocator.cc:1095] InUse at 716a03e00 of size 256 next 58\n",
      "2023-04-24 22:31:12.049190: I tensorflow/tsl/framework/bfc_allocator.cc:1095] InUse at 716a03f00 of size 256 next 59\n",
      "2023-04-24 22:31:12.049192: I tensorflow/tsl/framework/bfc_allocator.cc:1095] Free  at 716a04000 of size 256 next 79\n",
      "2023-04-24 22:31:12.049194: I tensorflow/tsl/framework/bfc_allocator.cc:1095] InUse at 716a04100 of size 256 next 61\n",
      "2023-04-24 22:31:12.049195: I tensorflow/tsl/framework/bfc_allocator.cc:1095] InUse at 716a04200 of size 256 next 62\n",
      "2023-04-24 22:31:12.049197: I tensorflow/tsl/framework/bfc_allocator.cc:1095] Free  at 716a04300 of size 14336 next 81\n",
      "2023-04-24 22:31:12.049201: I tensorflow/tsl/framework/bfc_allocator.cc:1095] InUse at 716a07b00 of size 7168 next 66\n",
      "2023-04-24 22:31:12.049202: I tensorflow/tsl/framework/bfc_allocator.cc:1095] Free  at 716a09700 of size 1536 next 84\n",
      "2023-04-24 22:31:12.049204: I tensorflow/tsl/framework/bfc_allocator.cc:1095] InUse at 716a09d00 of size 256 next 85\n",
      "2023-04-24 22:31:12.049206: I tensorflow/tsl/framework/bfc_allocator.cc:1095] Free  at 716a09e00 of size 512 next 87\n",
      "2023-04-24 22:31:12.049207: I tensorflow/tsl/framework/bfc_allocator.cc:1095] InUse at 716a0a000 of size 256 next 88\n",
      "2023-04-24 22:31:12.049209: I tensorflow/tsl/framework/bfc_allocator.cc:1095] Free  at 716a0a100 of size 36096 next 4\n",
      "2023-04-24 22:31:12.049211: I tensorflow/tsl/framework/bfc_allocator.cc:1095] InUse at 716a12e00 of size 38144 next 5\n",
      "2023-04-24 22:31:12.049213: I tensorflow/tsl/framework/bfc_allocator.cc:1095] InUse at 716a1c300 of size 65536 next 30\n",
      "2023-04-24 22:31:12.049215: I tensorflow/tsl/framework/bfc_allocator.cc:1095] InUse at 716a2c300 of size 123904 next 2\n",
      "2023-04-24 22:31:12.049216: I tensorflow/tsl/framework/bfc_allocator.cc:1095] InUse at 716a4a700 of size 151808 next 3\n",
      "2023-04-24 22:31:12.049218: I tensorflow/tsl/framework/bfc_allocator.cc:1095] InUse at 716a6f800 of size 3264000 next 16\n",
      "2023-04-24 22:31:12.049220: I tensorflow/tsl/framework/bfc_allocator.cc:1095] InUse at 716d8c600 of size 131072 next 24\n",
      "2023-04-24 22:31:12.049222: I tensorflow/tsl/framework/bfc_allocator.cc:1095] InUse at 716dac600 of size 131072 next 23\n",
      "2023-04-24 22:31:12.049224: I tensorflow/tsl/framework/bfc_allocator.cc:1095] InUse at 716dcc600 of size 1024000000 next 41\n",
      "2023-04-24 22:31:12.049227: I tensorflow/tsl/framework/bfc_allocator.cc:1095] InUse at 753e5c600 of size 131072 next 43\n",
      "2023-04-24 22:31:12.049228: I tensorflow/tsl/framework/bfc_allocator.cc:1095] InUse at 753e7c600 of size 65536 next 46\n",
      "2023-04-24 22:31:12.049230: I tensorflow/tsl/framework/bfc_allocator.cc:1095] Free  at 753e8c600 of size 116736 next 78\n",
      "2023-04-24 22:31:12.049232: I tensorflow/tsl/framework/bfc_allocator.cc:1095] InUse at 753ea8e00 of size 1759232 next 77\n",
      "2023-04-24 22:31:12.049233: I tensorflow/tsl/framework/bfc_allocator.cc:1095] Free  at 754056600 of size 1021665280 next 21\n",
      "2023-04-24 22:31:12.049235: I tensorflow/tsl/framework/bfc_allocator.cc:1095] InUse at 790eac600 of size 1024000000 next 20\n",
      "2023-04-24 22:31:12.049237: I tensorflow/tsl/framework/bfc_allocator.cc:1095] InUse at 7cdf3c600 of size 1024000000 next 42\n",
      "2023-04-24 22:31:12.049239: I tensorflow/tsl/framework/bfc_allocator.cc:1095] Free  at 80afcc600 of size 1522743808 next 18446744073709551615\n",
      "2023-04-24 22:31:12.049241: I tensorflow/tsl/framework/bfc_allocator.cc:1100]      Summary of in-use Chunks by size: \n",
      "2023-04-24 22:31:12.049244: I tensorflow/tsl/framework/bfc_allocator.cc:1103] 41 Chunks of size 256 totalling 10.2KiB\n",
      "2023-04-24 22:31:12.049246: I tensorflow/tsl/framework/bfc_allocator.cc:1103] 3 Chunks of size 512 totalling 1.5KiB\n",
      "2023-04-24 22:31:12.049248: I tensorflow/tsl/framework/bfc_allocator.cc:1103] 4 Chunks of size 1024 totalling 4.0KiB\n",
      "2023-04-24 22:31:12.049250: I tensorflow/tsl/framework/bfc_allocator.cc:1103] 1 Chunks of size 1280 totalling 1.2KiB\n",
      "2023-04-24 22:31:12.049252: I tensorflow/tsl/framework/bfc_allocator.cc:1103] 1 Chunks of size 7168 totalling 7.0KiB\n",
      "2023-04-24 22:31:12.049254: I tensorflow/tsl/framework/bfc_allocator.cc:1103] 1 Chunks of size 38144 totalling 37.2KiB\n",
      "2023-04-24 22:31:12.049256: I tensorflow/tsl/framework/bfc_allocator.cc:1103] 2 Chunks of size 65536 totalling 128.0KiB\n",
      "2023-04-24 22:31:12.049258: I tensorflow/tsl/framework/bfc_allocator.cc:1103] 1 Chunks of size 123904 totalling 121.0KiB\n",
      "2023-04-24 22:31:12.049260: I tensorflow/tsl/framework/bfc_allocator.cc:1103] 3 Chunks of size 131072 totalling 384.0KiB\n",
      "2023-04-24 22:31:12.049261: I tensorflow/tsl/framework/bfc_allocator.cc:1103] 1 Chunks of size 151808 totalling 148.2KiB\n",
      "2023-04-24 22:31:12.049263: I tensorflow/tsl/framework/bfc_allocator.cc:1103] 1 Chunks of size 1759232 totalling 1.68MiB\n",
      "2023-04-24 22:31:12.049265: I tensorflow/tsl/framework/bfc_allocator.cc:1103] 1 Chunks of size 3264000 totalling 3.11MiB\n",
      "2023-04-24 22:31:12.049267: I tensorflow/tsl/framework/bfc_allocator.cc:1103] 3 Chunks of size 1024000000 totalling 2.86GiB\n",
      "2023-04-24 22:31:12.049269: I tensorflow/tsl/framework/bfc_allocator.cc:1107] Sum Total of in-use chunks: 2.87GiB\n",
      "2023-04-24 22:31:12.049271: I tensorflow/tsl/framework/bfc_allocator.cc:1109] Total bytes in pool: 5622464512 memory_limit_: 5622464512 available bytes: 0 curr_region_allocation_bytes_: 11244929024\n",
      "2023-04-24 22:31:12.049277: I tensorflow/tsl/framework/bfc_allocator.cc:1114] Stats: \n",
      "Limit:                      5622464512\n",
      "InUse:                      3077885952\n",
      "MaxInUse:                   3089259520\n",
      "NumAllocs:                        6142\n",
      "MaxAllocSize:               1024000000\n",
      "Reserved:                            0\n",
      "PeakReserved:                        0\n",
      "LargestFreeBlock:                    0\n",
      "\n",
      "2023-04-24 22:31:12.049282: W tensorflow/tsl/framework/bfc_allocator.cc:497] *******************_________________*************************************___________________________\n",
      "2023-04-24 22:31:12.060824: I tensorflow/core/common_runtime/executor.cc:1197] [/job:localhost/replica:0/task:0/device:GPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): RESOURCE_EXHAUSTED: Out of memory while trying to allocate 2048000272 bytes.\n",
      "BufferAssignment OOM Debugging.\n",
      "BufferAssignment stats:\n",
      "             parameter allocation:    2.86GiB\n",
      "              constant allocation:         0B\n",
      "        maybe_live_out allocation:    2.86GiB\n",
      "     preallocated temp allocation:    1.91GiB\n",
      "  preallocated temp fragmentation:       124B (0.00%)\n",
      "                 total allocation:    4.77GiB\n",
      "              total fragmentation:  976.56MiB (19.99%)\n",
      "Peak buffers:\n",
      "\tBuffer 1:\n",
      "\t\tSize: 976.56MiB\n",
      "\t\tOperator: op_type=\"ResourceScatterAdd\" op_name=\"ResourceScatterAdd\" source_file=\"/home/ghufran/anaconda3/envs/siameseproj/lib/python3.9/site-packages/keras/optimizers/adam.py\" source_line=175\n",
      "\t\tXLA Label: fusion\n",
      "\t\tShape: f32[1000000,256]\n",
      "\t\t==========================\n",
      "\n",
      "\tBuffer 2:\n",
      "\t\tSize: 976.56MiB\n",
      "\t\tOperator: op_type=\"ResourceScatterAdd\" op_name=\"ResourceScatterAdd_1\" source_file=\"/home/ghufran/anaconda3/envs/siameseproj/lib/python3.9/site-packages/keras/optimizers/adam.py\" source_line=181\n",
      "\t\tXLA Label: fusion\n",
      "\t\tShape: f32[1000000,256]\n",
      "\t\t==========================\n",
      "\n",
      "\tBuffer 3:\n",
      "\t\tSize: 976.56MiB\n",
      "\t\tOperator: op_name=\"XLA_Args\"\n",
      "\t\tEntry Parameter Subshape: f32[1000000,256]\n",
      "\t\t==========================\n",
      "\n",
      "\tBuffer 4:\n",
      "\t\tSize: 976.56MiB\n",
      "\t\tOperator: op_name=\"XLA_Args\"\n",
      "\t\tEntry Parameter Subshape: f32[1000000,256]\n",
      "\t\t==========================\n",
      "\n",
      "\tBuffer 5:\n",
      "\t\tSize: 976.56MiB\n",
      "\t\tOperator: op_name=\"XLA_Args\"\n",
      "\t\tEntry Parameter Subshape: f32[1000000,256]\n",
      "\t\t==========================\n",
      "\n",
      "\tBuffer 6:\n",
      "\t\tSize: 1.68MiB\n",
      "\t\tOperator: op_name=\"XLA_Args\"\n",
      "\t\tEntry Parameter Subshape: f32[1718,256]\n",
      "\t\t==========================\n",
      "\n",
      "\tBuffer 7:\n",
      "\t\tSize: 6.7KiB\n",
      "\t\tOperator: op_name=\"XLA_Args\"\n",
      "\t\tEntry Parameter Subshape: s32[1718]\n",
      "\t\t==========================\n",
      "\n",
      "\tBuffer 8:\n",
      "\t\tSize: 24B\n",
      "\t\tXLA Label: tuple\n",
      "\t\tShape: (f32[1000000,256], f32[1000000,256], f32[1000000,256])\n",
      "\t\t==========================\n",
      "\n",
      "\tBuffer 9:\n",
      "\t\tSize: 16B\n",
      "\t\tOperator: op_type=\"Pow\" op_name=\"Pow\" source_file=\"/home/ghufran/anaconda3/envs/siameseproj/lib/python3.9/site-packages/keras/optimizers/adam.py\" source_line=163\n",
      "\t\tXLA Label: fusion\n",
      "\t\tShape: (f32[], f32[])\n",
      "\t\t==========================\n",
      "\n",
      "\tBuffer 10:\n",
      "\t\tSize: 8B\n",
      "\t\tOperator: op_name=\"XLA_Args\"\n",
      "\t\tEntry Parameter Subshape: s64[]\n",
      "\t\t==========================\n",
      "\n",
      "\tBuffer 11:\n",
      "\t\tSize: 8B\n",
      "\t\tOperator: op_name=\"XLA_Args\"\n",
      "\t\tEntry Parameter Subshape: s32[2]\n",
      "\t\t==========================\n",
      "\n",
      "\tBuffer 12:\n",
      "\t\tSize: 4B\n",
      "\t\tOperator: op_type=\"Pow\" op_name=\"Pow\" source_file=\"/home/ghufran/anaconda3/envs/siameseproj/lib/python3.9/site-packages/keras/optimizers/adam.py\" source_line=163\n",
      "\t\tXLA Label: fusion\n",
      "\t\tShape: f32[]\n",
      "\t\t==========================\n",
      "\n",
      "\tBuffer 13:\n",
      "\t\tSize: 4B\n",
      "\t\tXLA Label: parameter\n",
      "\t\tShape: f32[]\n",
      "\t\t==========================\n",
      "\n",
      "\tBuffer 14:\n",
      "\t\tSize: 4B\n",
      "\t\tXLA Label: parameter\n",
      "\t\tShape: f32[]\n",
      "\t\t==========================\n",
      "\n",
      "\tBuffer 15:\n",
      "\t\tSize: 4B\n",
      "\t\tOperator: op_name=\"XLA_Args\"\n",
      "\t\tEntry Parameter Subshape: f32[]\n",
      "\t\t==========================\n",
      "\n",
      "\n",
      "\t [[{{node StatefulPartitionedCall}}]]\n",
      "Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.\n",
      "\n"
     ]
    },
    {
     "ename": "ResourceExhaustedError",
     "evalue": "Graph execution error:\n\nDetected at node 'StatefulPartitionedCall' defined at (most recent call last):\n    File \"/home/ghufran/anaconda3/envs/siameseproj/lib/python3.9/runpy.py\", line 197, in _run_module_as_main\n      return _run_code(code, main_globals, None,\n    File \"/home/ghufran/anaconda3/envs/siameseproj/lib/python3.9/runpy.py\", line 87, in _run_code\n      exec(code, run_globals)\n    File \"/home/ghufran/anaconda3/envs/siameseproj/lib/python3.9/site-packages/ipykernel_launcher.py\", line 17, in <module>\n      app.launch_new_instance()\n    File \"/home/ghufran/anaconda3/envs/siameseproj/lib/python3.9/site-packages/traitlets/config/application.py\", line 992, in launch_instance\n      app.start()\n    File \"/home/ghufran/anaconda3/envs/siameseproj/lib/python3.9/site-packages/ipykernel/kernelapp.py\", line 711, in start\n      self.io_loop.start()\n    File \"/home/ghufran/anaconda3/envs/siameseproj/lib/python3.9/site-packages/tornado/platform/asyncio.py\", line 215, in start\n      self.asyncio_loop.run_forever()\n    File \"/home/ghufran/anaconda3/envs/siameseproj/lib/python3.9/asyncio/base_events.py\", line 601, in run_forever\n      self._run_once()\n    File \"/home/ghufran/anaconda3/envs/siameseproj/lib/python3.9/asyncio/base_events.py\", line 1905, in _run_once\n      handle._run()\n    File \"/home/ghufran/anaconda3/envs/siameseproj/lib/python3.9/asyncio/events.py\", line 80, in _run\n      self._context.run(self._callback, *self._args)\n    File \"/home/ghufran/anaconda3/envs/siameseproj/lib/python3.9/site-packages/ipykernel/kernelbase.py\", line 510, in dispatch_queue\n      await self.process_one()\n    File \"/home/ghufran/anaconda3/envs/siameseproj/lib/python3.9/site-packages/ipykernel/kernelbase.py\", line 499, in process_one\n      await dispatch(*args)\n    File \"/home/ghufran/anaconda3/envs/siameseproj/lib/python3.9/site-packages/ipykernel/kernelbase.py\", line 406, in dispatch_shell\n      await result\n    File \"/home/ghufran/anaconda3/envs/siameseproj/lib/python3.9/site-packages/ipykernel/kernelbase.py\", line 729, in execute_request\n      reply_content = await reply_content\n    File \"/home/ghufran/anaconda3/envs/siameseproj/lib/python3.9/site-packages/ipykernel/ipkernel.py\", line 411, in do_execute\n      res = shell.run_cell(\n    File \"/home/ghufran/anaconda3/envs/siameseproj/lib/python3.9/site-packages/ipykernel/zmqshell.py\", line 531, in run_cell\n      return super().run_cell(*args, **kwargs)\n    File \"/home/ghufran/anaconda3/envs/siameseproj/lib/python3.9/site-packages/IPython/core/interactiveshell.py\", line 3006, in run_cell\n      result = self._run_cell(\n    File \"/home/ghufran/anaconda3/envs/siameseproj/lib/python3.9/site-packages/IPython/core/interactiveshell.py\", line 3061, in _run_cell\n      result = runner(coro)\n    File \"/home/ghufran/anaconda3/envs/siameseproj/lib/python3.9/site-packages/IPython/core/async_helpers.py\", line 129, in _pseudo_sync_runner\n      coro.send(None)\n    File \"/home/ghufran/anaconda3/envs/siameseproj/lib/python3.9/site-packages/IPython/core/interactiveshell.py\", line 3266, in run_cell_async\n      has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n    File \"/home/ghufran/anaconda3/envs/siameseproj/lib/python3.9/site-packages/IPython/core/interactiveshell.py\", line 3445, in run_ast_nodes\n      if await self.run_code(code, result, async_=asy):\n    File \"/home/ghufran/anaconda3/envs/siameseproj/lib/python3.9/site-packages/IPython/core/interactiveshell.py\", line 3505, in run_code\n      exec(code_obj, self.user_global_ns, self.user_ns)\n    File \"/tmp/ipykernel_3739/2129847450.py\", line 1, in <module>\n      model.fit(train_ds, epochs=5, verbose=1)\n    File \"/home/ghufran/anaconda3/envs/siameseproj/lib/python3.9/site-packages/keras/utils/traceback_utils.py\", line 65, in error_handler\n      return fn(*args, **kwargs)\n    File \"/home/ghufran/anaconda3/envs/siameseproj/lib/python3.9/site-packages/keras/engine/training.py\", line 1685, in fit\n      tmp_logs = self.train_function(iterator)\n    File \"/home/ghufran/anaconda3/envs/siameseproj/lib/python3.9/site-packages/keras/engine/training.py\", line 1284, in train_function\n      return step_function(self, iterator)\n    File \"/home/ghufran/anaconda3/envs/siameseproj/lib/python3.9/site-packages/keras/engine/training.py\", line 1268, in step_function\n      outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"/home/ghufran/anaconda3/envs/siameseproj/lib/python3.9/site-packages/keras/engine/training.py\", line 1249, in run_step\n      outputs = model.train_step(data)\n    File \"/home/ghufran/anaconda3/envs/siameseproj/lib/python3.9/site-packages/keras/engine/training.py\", line 1054, in train_step\n      self.optimizer.minimize(loss, self.trainable_variables, tape=tape)\n    File \"/home/ghufran/anaconda3/envs/siameseproj/lib/python3.9/site-packages/keras/optimizers/optimizer.py\", line 543, in minimize\n      self.apply_gradients(grads_and_vars)\n    File \"/home/ghufran/anaconda3/envs/siameseproj/lib/python3.9/site-packages/keras/optimizers/optimizer.py\", line 1174, in apply_gradients\n      return super().apply_gradients(grads_and_vars, name=name)\n    File \"/home/ghufran/anaconda3/envs/siameseproj/lib/python3.9/site-packages/keras/optimizers/optimizer.py\", line 650, in apply_gradients\n      iteration = self._internal_apply_gradients(grads_and_vars)\n    File \"/home/ghufran/anaconda3/envs/siameseproj/lib/python3.9/site-packages/keras/optimizers/optimizer.py\", line 1200, in _internal_apply_gradients\n      return tf.__internal__.distribute.interim.maybe_merge_call(\n    File \"/home/ghufran/anaconda3/envs/siameseproj/lib/python3.9/site-packages/keras/optimizers/optimizer.py\", line 1250, in _distributed_apply_gradients_fn\n      distribution.extended.update(\n    File \"/home/ghufran/anaconda3/envs/siameseproj/lib/python3.9/site-packages/keras/optimizers/optimizer.py\", line 1245, in apply_grad_to_update_var\n      return self._update_step_xla(grad, var, id(self._var_key(var)))\nNode: 'StatefulPartitionedCall'\nOut of memory while trying to allocate 2048000272 bytes.\nBufferAssignment OOM Debugging.\nBufferAssignment stats:\n             parameter allocation:    2.86GiB\n              constant allocation:         0B\n        maybe_live_out allocation:    2.86GiB\n     preallocated temp allocation:    1.91GiB\n  preallocated temp fragmentation:       124B (0.00%)\n                 total allocation:    4.77GiB\n              total fragmentation:  976.56MiB (19.99%)\nPeak buffers:\n\tBuffer 1:\n\t\tSize: 976.56MiB\n\t\tOperator: op_type=\"ResourceScatterAdd\" op_name=\"ResourceScatterAdd\" source_file=\"/home/ghufran/anaconda3/envs/siameseproj/lib/python3.9/site-packages/keras/optimizers/adam.py\" source_line=175\n\t\tXLA Label: fusion\n\t\tShape: f32[1000000,256]\n\t\t==========================\n\n\tBuffer 2:\n\t\tSize: 976.56MiB\n\t\tOperator: op_type=\"ResourceScatterAdd\" op_name=\"ResourceScatterAdd_1\" source_file=\"/home/ghufran/anaconda3/envs/siameseproj/lib/python3.9/site-packages/keras/optimizers/adam.py\" source_line=181\n\t\tXLA Label: fusion\n\t\tShape: f32[1000000,256]\n\t\t==========================\n\n\tBuffer 3:\n\t\tSize: 976.56MiB\n\t\tOperator: op_name=\"XLA_Args\"\n\t\tEntry Parameter Subshape: f32[1000000,256]\n\t\t==========================\n\n\tBuffer 4:\n\t\tSize: 976.56MiB\n\t\tOperator: op_name=\"XLA_Args\"\n\t\tEntry Parameter Subshape: f32[1000000,256]\n\t\t==========================\n\n\tBuffer 5:\n\t\tSize: 976.56MiB\n\t\tOperator: op_name=\"XLA_Args\"\n\t\tEntry Parameter Subshape: f32[1000000,256]\n\t\t==========================\n\n\tBuffer 6:\n\t\tSize: 1.68MiB\n\t\tOperator: op_name=\"XLA_Args\"\n\t\tEntry Parameter Subshape: f32[1718,256]\n\t\t==========================\n\n\tBuffer 7:\n\t\tSize: 6.7KiB\n\t\tOperator: op_name=\"XLA_Args\"\n\t\tEntry Parameter Subshape: s32[1718]\n\t\t==========================\n\n\tBuffer 8:\n\t\tSize: 24B\n\t\tXLA Label: tuple\n\t\tShape: (f32[1000000,256], f32[1000000,256], f32[1000000,256])\n\t\t==========================\n\n\tBuffer 9:\n\t\tSize: 16B\n\t\tOperator: op_type=\"Pow\" op_name=\"Pow\" source_file=\"/home/ghufran/anaconda3/envs/siameseproj/lib/python3.9/site-packages/keras/optimizers/adam.py\" source_line=163\n\t\tXLA Label: fusion\n\t\tShape: (f32[], f32[])\n\t\t==========================\n\n\tBuffer 10:\n\t\tSize: 8B\n\t\tOperator: op_name=\"XLA_Args\"\n\t\tEntry Parameter Subshape: s64[]\n\t\t==========================\n\n\tBuffer 11:\n\t\tSize: 8B\n\t\tOperator: op_name=\"XLA_Args\"\n\t\tEntry Parameter Subshape: s32[2]\n\t\t==========================\n\n\tBuffer 12:\n\t\tSize: 4B\n\t\tOperator: op_type=\"Pow\" op_name=\"Pow\" source_file=\"/home/ghufran/anaconda3/envs/siameseproj/lib/python3.9/site-packages/keras/optimizers/adam.py\" source_line=163\n\t\tXLA Label: fusion\n\t\tShape: f32[]\n\t\t==========================\n\n\tBuffer 13:\n\t\tSize: 4B\n\t\tXLA Label: parameter\n\t\tShape: f32[]\n\t\t==========================\n\n\tBuffer 14:\n\t\tSize: 4B\n\t\tXLA Label: parameter\n\t\tShape: f32[]\n\t\t==========================\n\n\tBuffer 15:\n\t\tSize: 4B\n\t\tOperator: op_name=\"XLA_Args\"\n\t\tEntry Parameter Subshape: f32[]\n\t\t==========================\n\n\n\t [[{{node StatefulPartitionedCall}}]]\nHint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.\n [Op:__inference_train_function_7928]",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mResourceExhaustedError\u001b[0m                    Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[16], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_ds\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m5\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/siameseproj/lib/python3.9/site-packages/keras/utils/traceback_utils.py:70\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     67\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[1;32m     68\u001b[0m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[1;32m     69\u001b[0m     \u001b[38;5;66;03m# `tf.debugging.disable_traceback_filtering()`\u001b[39;00m\n\u001b[0;32m---> 70\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28mNone\u001b[39m\n\u001b[1;32m     71\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m     72\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[0;32m~/anaconda3/envs/siameseproj/lib/python3.9/site-packages/tensorflow/python/eager/execute.py:52\u001b[0m, in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     50\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m     51\u001b[0m   ctx\u001b[38;5;241m.\u001b[39mensure_initialized()\n\u001b[0;32m---> 52\u001b[0m   tensors \u001b[38;5;241m=\u001b[39m pywrap_tfe\u001b[38;5;241m.\u001b[39mTFE_Py_Execute(ctx\u001b[38;5;241m.\u001b[39m_handle, device_name, op_name,\n\u001b[1;32m     53\u001b[0m                                       inputs, attrs, num_outputs)\n\u001b[1;32m     54\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m core\u001b[38;5;241m.\u001b[39m_NotOkStatusException \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m     55\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[0;31mResourceExhaustedError\u001b[0m: Graph execution error:\n\nDetected at node 'StatefulPartitionedCall' defined at (most recent call last):\n    File \"/home/ghufran/anaconda3/envs/siameseproj/lib/python3.9/runpy.py\", line 197, in _run_module_as_main\n      return _run_code(code, main_globals, None,\n    File \"/home/ghufran/anaconda3/envs/siameseproj/lib/python3.9/runpy.py\", line 87, in _run_code\n      exec(code, run_globals)\n    File \"/home/ghufran/anaconda3/envs/siameseproj/lib/python3.9/site-packages/ipykernel_launcher.py\", line 17, in <module>\n      app.launch_new_instance()\n    File \"/home/ghufran/anaconda3/envs/siameseproj/lib/python3.9/site-packages/traitlets/config/application.py\", line 992, in launch_instance\n      app.start()\n    File \"/home/ghufran/anaconda3/envs/siameseproj/lib/python3.9/site-packages/ipykernel/kernelapp.py\", line 711, in start\n      self.io_loop.start()\n    File \"/home/ghufran/anaconda3/envs/siameseproj/lib/python3.9/site-packages/tornado/platform/asyncio.py\", line 215, in start\n      self.asyncio_loop.run_forever()\n    File \"/home/ghufran/anaconda3/envs/siameseproj/lib/python3.9/asyncio/base_events.py\", line 601, in run_forever\n      self._run_once()\n    File \"/home/ghufran/anaconda3/envs/siameseproj/lib/python3.9/asyncio/base_events.py\", line 1905, in _run_once\n      handle._run()\n    File \"/home/ghufran/anaconda3/envs/siameseproj/lib/python3.9/asyncio/events.py\", line 80, in _run\n      self._context.run(self._callback, *self._args)\n    File \"/home/ghufran/anaconda3/envs/siameseproj/lib/python3.9/site-packages/ipykernel/kernelbase.py\", line 510, in dispatch_queue\n      await self.process_one()\n    File \"/home/ghufran/anaconda3/envs/siameseproj/lib/python3.9/site-packages/ipykernel/kernelbase.py\", line 499, in process_one\n      await dispatch(*args)\n    File \"/home/ghufran/anaconda3/envs/siameseproj/lib/python3.9/site-packages/ipykernel/kernelbase.py\", line 406, in dispatch_shell\n      await result\n    File \"/home/ghufran/anaconda3/envs/siameseproj/lib/python3.9/site-packages/ipykernel/kernelbase.py\", line 729, in execute_request\n      reply_content = await reply_content\n    File \"/home/ghufran/anaconda3/envs/siameseproj/lib/python3.9/site-packages/ipykernel/ipkernel.py\", line 411, in do_execute\n      res = shell.run_cell(\n    File \"/home/ghufran/anaconda3/envs/siameseproj/lib/python3.9/site-packages/ipykernel/zmqshell.py\", line 531, in run_cell\n      return super().run_cell(*args, **kwargs)\n    File \"/home/ghufran/anaconda3/envs/siameseproj/lib/python3.9/site-packages/IPython/core/interactiveshell.py\", line 3006, in run_cell\n      result = self._run_cell(\n    File \"/home/ghufran/anaconda3/envs/siameseproj/lib/python3.9/site-packages/IPython/core/interactiveshell.py\", line 3061, in _run_cell\n      result = runner(coro)\n    File \"/home/ghufran/anaconda3/envs/siameseproj/lib/python3.9/site-packages/IPython/core/async_helpers.py\", line 129, in _pseudo_sync_runner\n      coro.send(None)\n    File \"/home/ghufran/anaconda3/envs/siameseproj/lib/python3.9/site-packages/IPython/core/interactiveshell.py\", line 3266, in run_cell_async\n      has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n    File \"/home/ghufran/anaconda3/envs/siameseproj/lib/python3.9/site-packages/IPython/core/interactiveshell.py\", line 3445, in run_ast_nodes\n      if await self.run_code(code, result, async_=asy):\n    File \"/home/ghufran/anaconda3/envs/siameseproj/lib/python3.9/site-packages/IPython/core/interactiveshell.py\", line 3505, in run_code\n      exec(code_obj, self.user_global_ns, self.user_ns)\n    File \"/tmp/ipykernel_3739/2129847450.py\", line 1, in <module>\n      model.fit(train_ds, epochs=5, verbose=1)\n    File \"/home/ghufran/anaconda3/envs/siameseproj/lib/python3.9/site-packages/keras/utils/traceback_utils.py\", line 65, in error_handler\n      return fn(*args, **kwargs)\n    File \"/home/ghufran/anaconda3/envs/siameseproj/lib/python3.9/site-packages/keras/engine/training.py\", line 1685, in fit\n      tmp_logs = self.train_function(iterator)\n    File \"/home/ghufran/anaconda3/envs/siameseproj/lib/python3.9/site-packages/keras/engine/training.py\", line 1284, in train_function\n      return step_function(self, iterator)\n    File \"/home/ghufran/anaconda3/envs/siameseproj/lib/python3.9/site-packages/keras/engine/training.py\", line 1268, in step_function\n      outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"/home/ghufran/anaconda3/envs/siameseproj/lib/python3.9/site-packages/keras/engine/training.py\", line 1249, in run_step\n      outputs = model.train_step(data)\n    File \"/home/ghufran/anaconda3/envs/siameseproj/lib/python3.9/site-packages/keras/engine/training.py\", line 1054, in train_step\n      self.optimizer.minimize(loss, self.trainable_variables, tape=tape)\n    File \"/home/ghufran/anaconda3/envs/siameseproj/lib/python3.9/site-packages/keras/optimizers/optimizer.py\", line 543, in minimize\n      self.apply_gradients(grads_and_vars)\n    File \"/home/ghufran/anaconda3/envs/siameseproj/lib/python3.9/site-packages/keras/optimizers/optimizer.py\", line 1174, in apply_gradients\n      return super().apply_gradients(grads_and_vars, name=name)\n    File \"/home/ghufran/anaconda3/envs/siameseproj/lib/python3.9/site-packages/keras/optimizers/optimizer.py\", line 650, in apply_gradients\n      iteration = self._internal_apply_gradients(grads_and_vars)\n    File \"/home/ghufran/anaconda3/envs/siameseproj/lib/python3.9/site-packages/keras/optimizers/optimizer.py\", line 1200, in _internal_apply_gradients\n      return tf.__internal__.distribute.interim.maybe_merge_call(\n    File \"/home/ghufran/anaconda3/envs/siameseproj/lib/python3.9/site-packages/keras/optimizers/optimizer.py\", line 1250, in _distributed_apply_gradients_fn\n      distribution.extended.update(\n    File \"/home/ghufran/anaconda3/envs/siameseproj/lib/python3.9/site-packages/keras/optimizers/optimizer.py\", line 1245, in apply_grad_to_update_var\n      return self._update_step_xla(grad, var, id(self._var_key(var)))\nNode: 'StatefulPartitionedCall'\nOut of memory while trying to allocate 2048000272 bytes.\nBufferAssignment OOM Debugging.\nBufferAssignment stats:\n             parameter allocation:    2.86GiB\n              constant allocation:         0B\n        maybe_live_out allocation:    2.86GiB\n     preallocated temp allocation:    1.91GiB\n  preallocated temp fragmentation:       124B (0.00%)\n                 total allocation:    4.77GiB\n              total fragmentation:  976.56MiB (19.99%)\nPeak buffers:\n\tBuffer 1:\n\t\tSize: 976.56MiB\n\t\tOperator: op_type=\"ResourceScatterAdd\" op_name=\"ResourceScatterAdd\" source_file=\"/home/ghufran/anaconda3/envs/siameseproj/lib/python3.9/site-packages/keras/optimizers/adam.py\" source_line=175\n\t\tXLA Label: fusion\n\t\tShape: f32[1000000,256]\n\t\t==========================\n\n\tBuffer 2:\n\t\tSize: 976.56MiB\n\t\tOperator: op_type=\"ResourceScatterAdd\" op_name=\"ResourceScatterAdd_1\" source_file=\"/home/ghufran/anaconda3/envs/siameseproj/lib/python3.9/site-packages/keras/optimizers/adam.py\" source_line=181\n\t\tXLA Label: fusion\n\t\tShape: f32[1000000,256]\n\t\t==========================\n\n\tBuffer 3:\n\t\tSize: 976.56MiB\n\t\tOperator: op_name=\"XLA_Args\"\n\t\tEntry Parameter Subshape: f32[1000000,256]\n\t\t==========================\n\n\tBuffer 4:\n\t\tSize: 976.56MiB\n\t\tOperator: op_name=\"XLA_Args\"\n\t\tEntry Parameter Subshape: f32[1000000,256]\n\t\t==========================\n\n\tBuffer 5:\n\t\tSize: 976.56MiB\n\t\tOperator: op_name=\"XLA_Args\"\n\t\tEntry Parameter Subshape: f32[1000000,256]\n\t\t==========================\n\n\tBuffer 6:\n\t\tSize: 1.68MiB\n\t\tOperator: op_name=\"XLA_Args\"\n\t\tEntry Parameter Subshape: f32[1718,256]\n\t\t==========================\n\n\tBuffer 7:\n\t\tSize: 6.7KiB\n\t\tOperator: op_name=\"XLA_Args\"\n\t\tEntry Parameter Subshape: s32[1718]\n\t\t==========================\n\n\tBuffer 8:\n\t\tSize: 24B\n\t\tXLA Label: tuple\n\t\tShape: (f32[1000000,256], f32[1000000,256], f32[1000000,256])\n\t\t==========================\n\n\tBuffer 9:\n\t\tSize: 16B\n\t\tOperator: op_type=\"Pow\" op_name=\"Pow\" source_file=\"/home/ghufran/anaconda3/envs/siameseproj/lib/python3.9/site-packages/keras/optimizers/adam.py\" source_line=163\n\t\tXLA Label: fusion\n\t\tShape: (f32[], f32[])\n\t\t==========================\n\n\tBuffer 10:\n\t\tSize: 8B\n\t\tOperator: op_name=\"XLA_Args\"\n\t\tEntry Parameter Subshape: s64[]\n\t\t==========================\n\n\tBuffer 11:\n\t\tSize: 8B\n\t\tOperator: op_name=\"XLA_Args\"\n\t\tEntry Parameter Subshape: s32[2]\n\t\t==========================\n\n\tBuffer 12:\n\t\tSize: 4B\n\t\tOperator: op_type=\"Pow\" op_name=\"Pow\" source_file=\"/home/ghufran/anaconda3/envs/siameseproj/lib/python3.9/site-packages/keras/optimizers/adam.py\" source_line=163\n\t\tXLA Label: fusion\n\t\tShape: f32[]\n\t\t==========================\n\n\tBuffer 13:\n\t\tSize: 4B\n\t\tXLA Label: parameter\n\t\tShape: f32[]\n\t\t==========================\n\n\tBuffer 14:\n\t\tSize: 4B\n\t\tXLA Label: parameter\n\t\tShape: f32[]\n\t\t==========================\n\n\tBuffer 15:\n\t\tSize: 4B\n\t\tOperator: op_name=\"XLA_Args\"\n\t\tEntry Parameter Subshape: f32[]\n\t\t==========================\n\n\n\t [[{{node StatefulPartitionedCall}}]]\nHint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.\n [Op:__inference_train_function_7928]"
     ]
    }
   ],
   "source": [
    "model.fit(train_ds, epochs=5, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bFh3K_bqMOIl"
   },
   "outputs": [],
   "source": [
    "model.evaluate(test_ds, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JEvE7OAaMOwP"
   },
   "outputs": [],
   "source": [
    "#### Antijoin topics with correlations data, since we don't have to predict those topics\n",
    "outer_joined = topics.merge(correlations, how='outer', left_on='id', right_on='topic_id', indicator=True)\n",
    "topics = outer_joined[(outer_joined._merge == 'left_only')].drop('_merge', axis=1)\n",
    "\n",
    "#### Fill missing values and concatenate text to features column \n",
    "topics = topics.fillna(\"\")\n",
    "topics_ids = topics.id.values\n",
    "topics_lang = topics.language\n",
    "topics_index = topics.index\n",
    "topics_features = topics[\"language\"] + ' ' + topics[\"channel\"] + ' ' + topics[\"title\"] + ' ' + topics[\"description\"]\n",
    "del topics\n",
    "\n",
    "#### Repeat for content, except we keep all content data\n",
    "content = content.fillna(\"\")\n",
    "content_ids = content.id.values\n",
    "content_index = content.index\n",
    "content_lang = content.language\n",
    "content_features = content[\"language\"] + ' ' + content[\"title\"] + ' ' + content[\"description\"] + ' ' + content[\"text\"]\n",
    "del content\n",
    "\n",
    "index_to_content = pd.Series(content_ids, index=content_index).to_dict()\n",
    "index_to_topic = pd.Series(topics_ids, index=topics_index).to_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6R6ri9SvMRXR"
   },
   "outputs": [],
   "source": [
    "#### Remove stopwords\n",
    "topics_features = topics_features.apply(remove_stopwords)\n",
    "content_features = content_features.apply(remove_stopwords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7NuFiXvFMVwG"
   },
   "outputs": [],
   "source": [
    "# #### Write predictions to output_file\n",
    "# THRESHOLD = 0.99994\n",
    "\n",
    "# output_file = \"submission.csv\"\n",
    "# f = open(output_file, 'w')\n",
    "\n",
    "# writer = csv.writer(f)\n",
    "# writer.writerow([\"topic_id\", \"content_ids\"])\n",
    "\n",
    "# for i in tqdm(topics_features.index):\n",
    "#     temp_content = tf.data.Dataset.from_tensor_slices(\n",
    "#         tf.cast(content_features[content_lang == topics_lang[i]], tf.string)\n",
    "#     )\n",
    "#     temp_topic = tf.data.Dataset.from_tensor_slices(\n",
    "#         tf.cast(np.repeat(topics_features[i], len(temp_content)), tf.string)\n",
    "#     )\n",
    "#     temp_ds = tf.data.Dataset.zip(((temp_topic, temp_content), ))\\\n",
    "#         .batch(batch_size=64)\\\n",
    "#             .cache()\\\n",
    "#                 .prefetch(tf.data.experimental.AUTOTUNE)\n",
    "#     matches = model.predict(temp_ds, verbose=0)\n",
    "#     matches = [i for i in range(len(matches)) if matches[i] > THRESHOLD]\n",
    "#     matches = \" \".join([index_to_content[x] for x in matches])\n",
    "#     writer.writerow([index_to_topic[i], matches])\n",
    "\n",
    "# #### Add given correlations data\n",
    "# writer.writerows([correlations.topic_id, correlations.content_ids])    \n",
    "\n",
    "# f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "THRESHOLD = 0.99994\n",
    "output_file = \"submission.csv\"\n",
    "\n",
    "# Create a function to apply the threshold and get the matches\n",
    "def get_matches(matches):\n",
    "    indices = np.where(matches > THRESHOLD)[0]\n",
    "    return \" \".join([index_to_content[x] for x in indices])\n",
    "\n",
    "# Vectorize the function to apply it on a batch of matches\n",
    "vectorized_get_matches = np.vectorize(get_matches, signature=\"(n)->()\")\n",
    "\n",
    "# Prepare a DataFrame to store the results\n",
    "results = []\n",
    "\n",
    "# Iterate over unique topic languages\n",
    "for lang in np.unique(topics_lang):\n",
    "    # Filter content features based on language\n",
    "    filtered_content_features = content_features[content_lang == lang]\n",
    "    \n",
    "    # Filter topic features and indices based on language\n",
    "    filtered_topic_features = topics_features[topics_lang == lang]\n",
    "    filtered_topic_indices = topics_features.index[topics_lang == lang]\n",
    "    \n",
    "    # Create a dataset with all combinations of filtered_topic_features and filtered_content_features\n",
    "    temp_topic = np.repeat(filtered_topic_features, len(filtered_content_features), axis=0)\n",
    "    temp_content = np.tile(filtered_content_features, (len(filtered_topic_features), 1))\n",
    "\n",
    "    temp_ds = tf.data.Dataset.from_tensor_slices((temp_topic, temp_content)).batch(batch_size=64).prefetch(tf.data.experimental.AUTOTUNE)\n",
    "    \n",
    "    # Perform predictions on the dataset\n",
    "    matches = model.predict(temp_ds, verbose=0)\n",
    "    \n",
    "    # Reshape the matches array to match the original topic and content dimensions\n",
    "    matches = matches.reshape(len(filtered_topic_features), len(filtered_content_features))\n",
    "    \n",
    "    # Apply the threshold and get the matches for each topic\n",
    "    matched_content_ids = vectorized_get_matches(matches)\n",
    "    \n",
    "    # Combine the topic indices and matched content IDs\n",
    "    results.extend(zip(filtered_topic_indices, matched_content_ids))\n",
    "\n",
    "# Add given correlations data\n",
    "results.extend(zip(correlations.topic_id, correlations.content_ids))\n",
    "\n",
    "# Save the results to a CSV file\n",
    "results_df = pd.DataFrame(results, columns=[\"topic_id\", \"content_ids\"])\n",
    "results_df.to_csv(output_file, index=False)\n"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
