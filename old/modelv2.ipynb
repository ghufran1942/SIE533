{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import pandas as pd\n",
    "from transformers import BertTokenizer, BertForSequenceClassification, AdamW, get_linear_schedule_with_warmup\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the CSV files\n",
    "topics_df = pd.read_csv(\"topics.csv\")\n",
    "correlations_df = pd.read_csv(\"correlations.csv\")\n",
    "\n",
    "# Extract relevant information\n",
    "# You'll need to update these lines based on the structure of your CSV files\n",
    "train_texts = topics_df[\"content\"].tolist()\n",
    "train_labels = topics_df[\"topic\"].tolist()\n",
    "correlations_texts = correlations_df[\"content\"].tolist()\n",
    "correlations_labels = correlations_df[\"topic\"].tolist()\n",
    "\n",
    "# Encode the data\n",
    "encoded_train_data = encode_data(train_texts, train_labels, tokenizer, max_seq_length, device)\n",
    "encoded_correlations_data = encode_data(correlations_texts, correlations_labels, tokenizer, max_seq_length, device)\n",
    "\n",
    "# Now you can use `encoded_correlations_data` for prediction or evaluation\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "\n",
    "# Tokenization and encoding\n",
    "def encode_data(texts, labels, tokenizer, max_seq_length):\n",
    "    input_ids, attention_masks, target_labels = [], [], []\n",
    "\n",
    "    for text, label in zip(texts, labels):\n",
    "        encoded = tokenizer.encode_plus(\n",
    "            text,\n",
    "            max_length=max_seq_length,\n",
    "            padding=\"max_length\",\n",
    "            truncation=True,\n",
    "            return_tensors=\"pt\",\n",
    "            return_attention_mask=True,\n",
    "        )\n",
    "        input_ids.append(encoded[\"input_ids\"])\n",
    "        attention_masks.append(encoded[\"attention_mask\"])\n",
    "        target_labels.append(label)\n",
    "\n",
    "    return {\n",
    "        \"input_ids\": torch.cat(input_ids, dim=0).to(device),\n",
    "        \"attention_masks\": torch.cat(attention_masks, dim=0).to(device),\n",
    "        \"labels\": torch.tensor(target_labels).to(device),\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "max_seq_length = 128\n",
    "encoded_train_data = encode_data(train_texts, train_labels, tokenizer, max_seq_length, device)\n",
    "encoded_val_data = encode_data(val_texts, val_labels, tokenizer, max_seq_length, device)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Model creation\n",
    "num_labels = len(set(train_labels))\n",
    "model = BertForSequenceClassification.from_pretrained(\"bert-base-uncased\", num_labels=num_labels)\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Create data loaders\n",
    "train_loader, val_loader = create_data_loaders(encoded_train_data, encoded_val_data, batch_size=16)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Fine-tuning\n",
    "epochs = 3\n",
    "learning_rate = 2e-5\n",
    "warmup_steps = 0\n",
    "optimizer = AdamW(model.parameters(), lr=learning_rate)\n",
    "scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=warmup_steps, num_training_steps=len(train_loader) * epochs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "train_model(model, optimizer, scheduler, train_loader, val_loader, epochs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Prediction function\n",
    "def predict_topic(text, model, tokenizer, max_seq_length):\n",
    "    model.eval()\n",
    "\n",
    "    encoded_text = tokenizer.encode_plus(\n",
    "        text,\n",
    "        max_length=max_seq_length,\n",
    "        padding=\"max_length\",\n",
    "        truncation=True,\n",
    "        return_tensors=\"pt\",\n",
    "        return_attention_mask=True,\n",
    "    )\n",
    "\n",
    "    with torch.no_grad():\n",
    "        input_ids = encoded_text[\"input_ids\"].to(device)\n",
    "        attention_mask = encoded_text[\"attention_mask\"].to(device)\n",
    "        output = model(input_ids, attention_mask=attention_mask)\n",
    "        probabilities = torch.nn.functional.softmax(output.logits, dim=1)\n",
    "\n",
    "    predicted_topic = predict_topic(input_text, model, tokenizer, max_seq_length, device)\n",
    "\n",
    "    return predicted_topic\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "input_text = \"Sample text to predict the topic\"\n",
    "predicted_topic = predict_topic(input_text, model, tokenizer, max_seq_length)\n",
    "print(f\"Predicted topic: {predicted_topic}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
